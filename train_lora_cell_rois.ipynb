{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMnFqiO2wonPVwLapO90YYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8803443f21434a6c9676a5573081f0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10af86fd781e4ad7a6c47ea8a83f0ff1",
              "IPY_MODEL_e5e39bcb59c24d36b2fa3ebdd2f298c1",
              "IPY_MODEL_178982ef8ed742dbbb595eb850a7a1b7"
            ],
            "layout": "IPY_MODEL_1317f161c37742afa50ade84f9a176fb"
          }
        },
        "10af86fd781e4ad7a6c47ea8a83f0ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17fb7b79ecf4c58b56728713431291c",
            "placeholder": "​",
            "style": "IPY_MODEL_d7131a1bffbd4e2ab8bbc851215dd2da",
            "value": "Steps: 100%"
          }
        },
        "e5e39bcb59c24d36b2fa3ebdd2f298c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9226d0fcb964db0bfa03d2228baf5f1",
            "max": 11100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1f371890923453fb933b6fd4d4b3035",
            "value": 11100
          }
        },
        "178982ef8ed742dbbb595eb850a7a1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cd93ed7be349c79b30801a07e709d0",
            "placeholder": "​",
            "style": "IPY_MODEL_953f05db332c44d5b631cf1477be2e40",
            "value": " 11100/11100 [1:39:13&lt;00:00,  1.90it/s, loss=0.0248, lr=0]"
          }
        },
        "1317f161c37742afa50ade84f9a176fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17fb7b79ecf4c58b56728713431291c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7131a1bffbd4e2ab8bbc851215dd2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9226d0fcb964db0bfa03d2228baf5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f371890923453fb933b6fd4d4b3035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98cd93ed7be349c79b30801a07e709d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953f05db332c44d5b631cf1477be2e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kktsuji/train-lora-cell-rois/blob/main/train_lora_cell_rois.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPzC39ZGci3G",
        "outputId": "1636129f-f4f4-47ce-cd07-5946866a2a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Settings\n",
        "\n",
        "# RESOLUTION = \"40\" # original size\n",
        "# RESOLUTION = \"256\"\n",
        "RESOLUTION = \"512\" # recommended for sd-1.x\n",
        "\n",
        "NUM_TRAIN_EPOCHS = \"300\"\n",
        "\n",
        "out_dir_name = f\"cell-rois_lora_resolution-{RESOLUTION}_epochs-{NUM_TRAIN_EPOCHS}\""
      ],
      "metadata": {
        "id": "aOMYXJ_44urC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "out_dir_path = f\"/content/drive/MyDrive/Research/LoRAs/{out_dir_name}/\"\n",
        "log_dir_path = f\"/content/drive/MyDrive/Research/LoRAs/{out_dir_name}/logs/\"\n",
        "if not os.path.exists(out_dir_path):\n",
        "  os.makedirs(out_dir_path)\n",
        "if not os.path.exists(log_dir_path):\n",
        "  os.makedirs(log_dir_path)\n",
        "print(out_dir_path)\n",
        "print(log_dir_path)\n",
        "\n",
        "base_data_dir_path = \"/content/drive/MyDrive/Research/Data/250606_cell_rois/pseudo_rgb/\"\n",
        "print(base_data_dir_path)\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "model_dir_path = \"/content/drive/MyDrive/Research/Models/\" + model_id\n",
        "print(model_dir_path)\n",
        "\n",
        "if (os.path.exists(out_dir_path) or\n",
        "  os.path.exists(log_dir_path) or\n",
        "  os.path.exists(base_data_dir_path) or\n",
        "  os.path.exists(model_dir_path)):\n",
        "  print(\"All directories exist\")\n",
        "else:\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()\n",
        "  raise Exception(\"Directory not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kv_xTpGeVqE",
        "outputId": "7acba1b8-360f-426c-87db-3bdbc682edd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/LoRAs/cell-rois_lora_resolution-512_epochs-300/\n",
            "/content/drive/MyDrive/Research/LoRAs/cell-rois_lora_resolution-512_epochs-300/logs/\n",
            "/content/drive/MyDrive/Research/Data/250606_cell_rois/pseudo_rgb/\n",
            "/content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4\n",
            "All directories exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(model_dir_path):\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  import torch\n",
        "  os.makedirs(model_dir_path)\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
        "  pipe.save_pretrained(model_dir_path)"
      ],
      "metadata": {
        "id": "Plomt4WcKFHp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b develop https://github.com/kktsuji/peft.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('./peft/examples/stable_diffusion')\n",
        "import train_dreambooth\n",
        "\n",
        "print(train_dreambooth.UNET_TARGET_MODULES)\n",
        "\n",
        "# for \"CompVis/stable-diffusion-v1-4\"\n",
        "unet_target_modules = [\n",
        "    # Convolution layers\n",
        "    \"conv_in\", \"conv1\", \"conv2\", \"conv_out\",\n",
        "    \"conv_shortcut\", \"conv\",\n",
        "    # Linear layers\n",
        "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",\n",
        "    \"linear_1\", \"linear_2\", \"proj_in\", \"proj_out\", \"proj\",\n",
        "    \"time_emb_proj\", \"ff.net.0.proj\", \"ff.net.2\",\n",
        "]\n",
        "\n",
        "train_dreambooth.UNET_TARGET_MODULES = unet_target_modules\n",
        "print(train_dreambooth.UNET_TARGET_MODULES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01v6bCHo-Zgb",
        "outputId": "4b354e2b-1282-4a53-a7ec-d39555f93a03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'peft'...\n",
            "remote: Enumerating objects: 11539, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (150/150), done.\u001b[K\n",
            "remote: Total 11539 (delta 153), reused 70 (delta 70), pack-reused 11319 (from 2)\u001b[K\n",
            "Receiving objects: 100% (11539/11539), 17.00 MiB | 8.25 MiB/s, done.\n",
            "Resolving deltas: 100% (7886/7886), done.\n",
            "['to_q', 'to_k', 'to_v', 'proj', 'proj_in', 'proj_out', 'conv', 'conv1', 'conv2', 'conv_shortcut', 'to_out.0', 'time_emb_proj', 'ff.net.2']\n",
            "['conv_in', 'conv1', 'conv2', 'conv_out', 'conv_shortcut', 'conv', 'to_q', 'to_k', 'to_v', 'to_out.0', 'linear_1', 'linear_2', 'proj_in', 'proj_out', 'proj', 'time_emb_proj', 'ff.net.0.proj', 'ff.net.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_args = [\n",
        "    \"--pretrained_model_name_or_path\", model_dir_path,\n",
        "    \"--instance_data_dir\", base_data_dir_path,\n",
        "    \"--instance_prompt\", \"rds\",\n",
        "    \"--seed\", \"0\",\n",
        "    \"--resolution\", RESOLUTION,\n",
        "    \"--output_dir\", out_dir_path,\n",
        "    \"--num_train_epochs\", NUM_TRAIN_EPOCHS,\n",
        "    \"--lr_scheduler\", \"cosine\",\n",
        "    \"--lr_warmup_steps\", \"5\",\n",
        "    \"--learning_rate\", \"1e-4\",\n",
        "    \"--logging_dir\", \"logs\",\n",
        "    \"--report_to\", \"tensorboard\",\n",
        "    \"lora\",\n",
        "    \"--unet_r\", \"16\",\n",
        "    \"--unet_alpha\", \"16\"\n",
        "]\n",
        "\n",
        "args = train_dreambooth.parse_args(input_args)\n",
        "train_dreambooth.main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8803443f21434a6c9676a5573081f0cd",
            "10af86fd781e4ad7a6c47ea8a83f0ff1",
            "e5e39bcb59c24d36b2fa3ebdd2f298c1",
            "178982ef8ed742dbbb595eb850a7a1b7",
            "1317f161c37742afa50ade84f9a176fb",
            "b17fb7b79ecf4c58b56728713431291c",
            "d7131a1bffbd4e2ab8bbc851215dd2da",
            "b9226d0fcb964db0bfa03d2228baf5f1",
            "d1f371890923453fb933b6fd4d4b3035",
            "98cd93ed7be349c79b30801a07e709d0",
            "953f05db332c44d5b631cf1477be2e40"
          ]
        },
        "id": "BGQ8iM4g7OoX",
        "outputId": "63a41c31-b2a1-4470-9d5c-1ff6b526225a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 16,931,456 || all params: 876,452,420 || trainable%: 1.9318\n",
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): UNet2DConditionModel(\n",
            "      (conv_in): lora.Conv2d(\n",
            "        (base_layer): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Identity()\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "        (lora_magnitude_vector): ModuleDict()\n",
            "      )\n",
            "      (time_proj): Timesteps()\n",
            "      (time_embedding): TimestepEmbedding(\n",
            "        (linear_1): lora.Linear(\n",
            "          (base_layer): Linear(in_features=320, out_features=1280, bias=True)\n",
            "          (lora_dropout): ModuleDict(\n",
            "            (default): Identity()\n",
            "          )\n",
            "          (lora_A): ModuleDict(\n",
            "            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "          )\n",
            "          (lora_B): ModuleDict(\n",
            "            (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "          )\n",
            "          (lora_embedding_A): ParameterDict()\n",
            "          (lora_embedding_B): ParameterDict()\n",
            "          (lora_magnitude_vector): ModuleDict()\n",
            "        )\n",
            "        (act): SiLU()\n",
            "        (linear_2): lora.Linear(\n",
            "          (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora_dropout): ModuleDict(\n",
            "            (default): Identity()\n",
            "          )\n",
            "          (lora_A): ModuleDict(\n",
            "            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "          )\n",
            "          (lora_B): ModuleDict(\n",
            "            (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "          )\n",
            "          (lora_embedding_A): ParameterDict()\n",
            "          (lora_embedding_B): ParameterDict()\n",
            "          (lora_magnitude_vector): ModuleDict()\n",
            "        )\n",
            "      )\n",
            "      (down_blocks): ModuleList(\n",
            "        (0): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=5120, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): DownBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (up_blocks): ModuleList(\n",
            "        (0): UpBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-2): 3 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=5120, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1-2): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (mid_block): UNetMidBlock2DCrossAttn(\n",
            "        (attentions): ModuleList(\n",
            "          (0): Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_k): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_k): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (time_emb_proj): lora.Linear(\n",
            "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "      (conv_act): SiLU()\n",
            "      (conv_out): lora.Conv2d(\n",
            "        (base_layer): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Identity()\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "        (lora_magnitude_vector): ModuleDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/11100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8803443f21434a6c9676a5573081f0cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4151\n",
            "GPU Memory consumed at the end of the train (end-begin): 163\n",
            "GPU Peak Memory consumed during the train (max-begin): 7554\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 1573\n",
            "CPU Memory consumed at the end of the train (end-begin): 733\n",
            "CPU Peak Memory consumed during the train (max-begin): 745\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7410\n",
            "GPU Total Peak Memory consumed during the train (max): 11725\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2307\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2319\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7410\n",
            "GPU Total Peak Memory consumed during the train (max): 11725\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7410\n",
            "GPU Total Peak Memory consumed during the train (max): 11725\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2308\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2320\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2309\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2321\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n",
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2310\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2322\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAjGdJREFUeJzt3Xd4U2XfB/Bv2tKWMspuCxbKkiF7iqDgY6UgKigq8OiDoC8qiIqoCMpGZYqIICgO9lQERXahzNJSoGWX1UU3Ld07Oe8fpaFpkzTjJOck+X6ui0ubnJxzn33/7qkQBEEAERERERERmcVJ6gQQERERERHZAwZXREREREREImBwRUREREREJAIGV0RERERERCJgcEVERERERCQCBldEREREREQiYHBFREREREQkAgZXREREREREImBwRUREREREJAIGV0REZFPGjBkDPz8/k347e/ZsKBQKcRNERET0AIMrIiIShUKhMOhfUFCQ1EmVxJgxY1CzZk2pk0FERBakEARBkDoRRERk+zZu3Kjx9/r163Ho0CFs2LBB4/Nnn30WXl5eJm+nuLgYKpUKbm5uRv+2pKQEJSUlcHd3N3n7phozZgz++OMP5OTkWH3bRERkHS5SJ4CIiOzDG2+8ofH3mTNncOjQoUqfV5SXlwcPDw+Dt1OtWjWT0gcALi4ucHHhq4+IiCyDzQKJiMhqBgwYgA4dOuDcuXN46qmn4OHhgS+++AIAsHv3bgwZMgSNGzeGm5sbWrZsiXnz5kGpVGqso2Kfq+joaCgUCixZsgQ///wzWrZsCTc3N/Ts2RNnz57V+K22PlcKhQITJ07Erl270KFDB7i5ueGxxx7D/v37K6U/KCgIPXr0gLu7O1q2bImffvpJ9H5cO3bsQPfu3VG9enU0aNAAb7zxBuLj4zWWSUpKwtixY/HII4/Azc0NPj4+GDp0KKKjo9XLhIWFISAgAA0aNED16tXRvHlzvPXWW6Klk4iIKmPxHRERWVVaWhoGDx6MkSNH4o033lA3EVy7di1q1qyJyZMno2bNmjhy5AhmzpyJrKwsLF68uMr1bt68GdnZ2Xj33XehUCiwaNEivPzyy7hz506VtV0nT57Ezp07MWHCBNSqVQvLly/H8OHDERsbi/r16wMALly4gEGDBsHHxwdz5syBUqnE3Llz0bBhQ/MPygNr167F2LFj0bNnT8yfPx/Jycn4/vvvcerUKVy4cAF16tQBAAwfPhxXrlzBBx98AD8/P6SkpODQoUOIjY1V/z1w4EA0bNgQU6dORZ06dRAdHY2dO3eKllYiItJCICIisoD3339fqPia6d+/vwBAWL16daXl8/LyKn327rvvCh4eHkJBQYH6szfffFNo1qyZ+u+oqCgBgFC/fn0hPT1d/fnu3bsFAMI///yj/mzWrFmV0gRAcHV1FW7duqX+LCIiQgAg/PDDD+rPXnjhBcHDw0OIj49Xf3bz5k3BxcWl0jq1efPNN4UaNWro/L6oqEho1KiR0KFDByE/P1/9+Z49ewQAwsyZMwVBEIT79+8LAITFixfrXNdff/0lABDOnj1bZbqIiEg8bBZIRERW5ebmhrFjx1b6vHr16ur/z87Oxr179/Dkk08iLy8P169fr3K9I0aMQN26ddV/P/nkkwCAO3fuVPlbf39/tGzZUv13p06dULt2bfVvlUolDh8+jGHDhqFx48bq5Vq1aoXBgwdXuX5DhIWFISUlBRMmTNAYcGPIkCFo27Yt/v33XwClx8nV1RVBQUG4f/++1nWV1XDt2bMHxcXFoqSPiIiqxuCKiIisqkmTJnB1da30+ZUrV/DSSy/B09MTtWvXRsOGDdWDYWRmZla53qZNm2r8XRZo6QpA9P227Pdlv01JSUF+fj5atWpVaTltn5kiJiYGANCmTZtK37Vt21b9vZubGxYuXIh9+/bBy8sLTz31FBYtWoSkpCT18v3798fw4cMxZ84cNGjQAEOHDsXvv/+OwsJCUdJKRETaMbgiIiKrKl9DVSYjIwP9+/dHREQE5s6di3/++QeHDh3CwoULAQAqlarK9To7O2v9XDBgxhFzfiuFSZMm4caNG5g/fz7c3d0xY8YMtGvXDhcuXABQOkjHH3/8geDgYEycOBHx8fF466230L17dw4FT0RkQQyuiIhIckFBQUhLS8PatWvx0Ucf4fnnn4e/v79GMz8pNWrUCO7u7rh161al77R9ZopmzZoBACIjIyt9FxkZqf6+TMuWLfHJJ5/g4MGDuHz5MoqKivDtt99qLPP444/j66+/RlhYGDZt2oQrV65g69atoqSXiIgqY3BFRESSK6s5Kl9TVFRUhB9//FGqJGlwdnaGv78/du3ahYSEBPXnt27dwr59+0TZRo8ePdCoUSOsXr1ao/nevn37cO3aNQwZMgRA6bxgBQUFGr9t2bIlatWqpf7d/fv3K9W6denSBQDYNJCIyII4FDsREUnuiSeeQN26dfHmm2/iww8/hEKhwIYNG2TVLG/27Nk4ePAg+vbti/Hjx0OpVGLFihXo0KEDwsPDDVpHcXExvvrqq0qf16tXDxMmTMDChQsxduxY9O/fH6NGjVIPxe7n54ePP/4YAHDjxg0888wzeO2119C+fXu4uLjgr7/+QnJyMkaOHAkAWLduHX788Ue89NJLaNmyJbKzs7FmzRrUrl0bzz33nGjHhIiINDG4IiIiydWvXx979uzBJ598gunTp6Nu3bp444038MwzzyAgIEDq5AEAunfvjn379uHTTz/FjBkz4Ovri7lz5+LatWsGjWYIlNbGzZgxo9LnLVu2xIQJEzBmzBh4eHhgwYIF+Pzzz1GjRg289NJLWLhwoXoEQF9fX4waNQqBgYHYsGEDXFxc0LZtW2zfvh3Dhw8HUDqgRWhoKLZu3Yrk5GR4enqiV69e2LRpE5o3by7aMSEiIk0KQU7FgkRERDZm2LBhuHLlCm7evCl1UoiISGLsc0VERGSg/Px8jb9v3ryJvXv3YsCAAdIkiIiIZIU1V0RERAby8fHBmDFj0KJFC8TExGDVqlUoLCzEhQsX0Lp1a6mTR0REEmOfKyIiIgMNGjQIW7ZsQVJSEtzc3NCnTx988803DKyIiAgAa66IiIiIiIhEwT5XREREREREImBwRUREREREJAL2udJCpVIhISEBtWrVgkKhkDo5REREREQkEUEQkJ2djcaNG8PJSX/dFIMrLRISEuDr6yt1MoiIiIiISCbi4uLwyCOP6F2GwZUWtWrVAlB6AGvXri1xaoiIiIiISCpZWVnw9fVVxwj6MLjSoqwpYO3atRlcERERERGRQd2FOKAFERERERGRCBhcERERERERiYDBFRERERERkQjY54qIiIiIyE4IgoCSkhIolUqpk2IznJ2d4eLiIsoUTAyuiIiIiIjsQFFRERITE5GXlyd1UmyOh4cHfHx84OrqatZ6GFwREREREdk4lUqFqKgoODs7o3HjxnB1dRWlJsbeCYKAoqIipKamIioqCq1bt65yomB9GFwREREREdm4oqIiqFQq+Pr6wsPDQ+rk2JTq1aujWrVqiImJQVFREdzd3U1eFwe0ICIiIiKyE+bUujgysY4bjz4REREREZEIGFwRERERERGJgMEVERERERGRCBhcERERERGRZMaMGYNhw4ZJnQxRMLgiIiIiIiISAYMrInJ4t1JyMPq3UJyLSZc6KURERKIQBAF5RSWS/BMEQbT9OHbsGHr16gU3Nzf4+Phg6tSpKCkpUX//xx9/oGPHjqhevTrq168Pf39/5ObmAgCCgoLQq1cv1KhRA3Xq1EHfvn0RExMjWtq04TxXROTw/m/dWUSn5eH4jVRELxgidXKIiIjMll+sRPuZByTZ9tW5AfBwNT/MiI+Px3PPPYcxY8Zg/fr1uH79OsaNGwd3d3fMnj0biYmJGDVqFBYtWoSXXnoJ2dnZOHHiBARBQElJCYYNG4Zx48Zhy5YtKCoqQmhoqMUnVmZwRUQOLyGjQOokEBERUQU//vgjfH19sWLFCigUCrRt2xYJCQn4/PPPMXPmTCQmJqKkpAQvv/wymjVrBgDo2LEjACA9PR2ZmZl4/vnn0bJlSwBAu3btLJ5mWQRXK1euxOLFi5GUlITOnTvjhx9+QK9evbQuu3PnTnzzzTe4desWiouL0bp1a3zyySf43//+p15GEATMmjULa9asQUZGBvr27YtVq1ahdevW1tolIiIiIiLJVK/mjKtzAyTbthiuXbuGPn36aNQ29e3bFzk5Obh79y46d+6MZ555Bh07dkRAQAAGDhyIV155BXXr1kW9evUwZswYBAQE4Nlnn4W/vz9ee+01+Pj4iJI2XSTvc7Vt2zZMnjwZs2bNwvnz59G5c2cEBAQgJSVF6/L16tXDl19+ieDgYFy8eBFjx47F2LFjceDAw2rPRYsWYfny5Vi9ejVCQkJQo0YNBAQEoKCApdNEREREZP8UCgU8XF0k+WfppndlnJ2dcejQIezbtw/t27fHDz/8gDZt2iAqKgoA8PvvvyM4OBhPPPEEtm3bhkcffRRnzpyxaJokD66WLl2KcePGYezYsWjfvj1Wr14NDw8P/Pbbb1qXHzBgAF566SW0a9cOLVu2xEcffYROnTrh5MmTAEprrZYtW4bp06dj6NCh6NSpE9avX4+EhATs2rXLintGRERERESmateuHYKDgzUGyDh16hRq1aqFRx55BEBpENm3b1/MmTMHFy5cgKurK/766y/18l27dsW0adNw+vRpdOjQAZs3b7ZomiUNroqKinDu3Dn4+/urP3NycoK/vz+Cg4Or/L0gCAgMDERkZCSeeuopAEBUVBSSkpI01unp6YnevXvrXGdhYSGysrI0/hERERERkXVkZmYiPDxc498777yDuLg4fPDBB7h+/Tp2796NWbNmYfLkyXByckJISAi++eYbhIWFITY2Fjt37kRqairatWuHqKgoTJs2DcHBwYiJicHBgwdx8+ZNi/e7krTP1b1796BUKuHl5aXxuZeXF65fv67zd5mZmWjSpAkKCwvh7OyMH3/8Ec8++ywAICkpSb2Oiuss+66i+fPnY86cOebsChERERERmSgoKAhdu3bV+Oztt9/G3r178dlnn6Fz586oV68e3n77bUyfPh0AULt2bRw/fhzLli1DVlYWmjVrhm+//RaDBw9GcnIyrl+/jnXr1iEtLQ0+Pj54//338e6771p0P2QxoIWxatWqhfDwcOTk5CAwMBCTJ09GixYtMGDAAJPWN23aNEyePFn9d1ZWFnx9fUVKLRERERER6bJ27VqsXbtW5/ehoaFaP2/Xrh3279+v9TsvLy+N5oHWImlw1aBBAzg7OyM5OVnj8+TkZHh7e+v8nZOTE1q1agUA6NKlC65du4b58+djwIAB6t8lJydrjAaSnJyMLl26aF2fm5sb3NzczNwbIiIiIiJyZJL2uXJ1dUX37t0RGBio/kylUiEwMBB9+vQxeD0qlQqFhYUAgObNm8Pb21tjnVlZWQgJCTFqnURERERERMaQvFng5MmT8eabb6JHjx7o1asXli1bhtzcXIwdOxYAMHr0aDRp0gTz588HUNo/qkePHmjZsiUKCwuxd+9ebNiwAatWrQJQOmLIpEmT8NVXX6F169Zo3rw5ZsyYgcaNG2PYsGFS7SYREREREdk5yYOrESNGIDU1FTNnzkRSUhK6dOmC/fv3qwekiI2NhZPTwwq23NxcTJgwAXfv3kX16tXRtm1bbNy4ESNGjFAvM2XKFOTm5uKdd95BRkYG+vXrh/3798Pd3d3q+0dENsA603EQERGRnVMI5QeOJwClzQg9PT2RmZmJ2rVrS50cIrKwR6fvQ1GJCgAQvWCIxKkhIiIyXkFBAaKiouDn54fq1atLnRybk5+fj+joaDRv3rxShYwxsYHkkwgTEREREZF5qlWrBgDIy8uTOCW2qey4lR1HU0neLJCIiIiIiMzj7OyMOnXqICUlBQDg4eEBhYLt3qsiCALy8vKQkpKCOnXqwNnZ2az1MbgiIiIiIrIDZVMSlQVYZLg6deronQrKUAyuiIiIiIjsgEKhgI+PDxo1aoTi4mKpk2MzqlWrZnaNVRkGV0REREREdsTZ2Vm0YIGMwwEtiIiIiIiIRMDgiqiCdaejsfRgpNTJICIiIiIbw2aBRBXM+vsKAGBo1yZo2bCmxKkhIiIiIlvBmisiHfIKlVIngYiIiIhsCIMrIiIiIiIiETC4IlkpUaqQV1QidTKIiIiIiIzG4IpkZeCy42g/8wAy8zg3AxERERHZFgZXJCt3UnMBACFRaRKnhIiIiIjIOAyuiIiIiIiIRMDgiohkTxAEqZNAREREVCUGV0Qka/P3XkPvbwKRml0odVKIiIiI9GJwRUSy9tPxO0jJLsQvJ+5InRQiIiIivRhcEZFtUEidACIiIiL9GFwRERERERGJgMEVERERERGRCBhcEZHDY4tDIiIiEgODKyIiIiIiIhEwuCIiIiIiIhIBgysiIiIiIiIRMLgiIiIiIiISAYMrIrINgtQJICIiItKPwRUREVmNIDBKJiIi+8XgioiIrGLvpUT0/DoQoVHpUieFiIjIIhhcOYgj15Ox4shNlhoTacG7wjombDqPezmFGPN7qNRJISIisggXqRNA1vHW2jAAwGNNPPF0m0YSp4aIHJlSxXCWiIjsE2uuHExSZoHUSTAIs15kTQqpE0Cy8uvJKKwKui11MoiIyAax5oqIiOiBgmIl5u25CgB4pfsjaFjLTeIUERGRLWHNFckSaxKISAqqcv1SC0uUEqaEiIhsEYMrIiIiIiIiETC4IiIiIiIiEgGDKyIiIiIiIhEwuCIiIqviaKBERGSvGFwRERERERGJgMEVERERERGRCBhcORgOcU5EREREZBkMroiIiIiMcDUhC6N/C8Wlu5lSJ4WIZIbBFRE5PAWrdK2Kh5ts3ag1Z3D8RipeXnVK6qQQkcwwuCIiIiIyQmZ+MQCgWMmxL4lIE4MrIiIiIiIiETC4IiKbwPJh+8FzSURE9orBFRER0QMCIz8iIjIDgysiIiIiIiIRyCK4WrlyJfz8/ODu7o7evXsjNDRU57Jr1qzBk08+ibp166Ju3brw9/evtPyYMWOgUCg0/g0aNMjSu0FERHZEwWEkiYjISJIHV9u2bcPkyZMxa9YsnD9/Hp07d0ZAQABSUlK0Lh8UFIRRo0bh6NGjCA4Ohq+vLwYOHIj4+HiN5QYNGoTExET1vy1btlhjd4iIyE4IbCNIRERGkjy4Wrp0KcaNG4exY8eiffv2WL16NTw8PPDbb79pXX7Tpk2YMGECunTpgrZt2+KXX36BSqVCYGCgxnJubm7w9vZW/6tbt67ONBQWFiIrK0vjH0mLWRqyJuahqQwrq4iIyBySBldFRUU4d+4c/P391Z85OTnB398fwcHBBq0jLy8PxcXFqFevnsbnQUFBaNSoEdq0aYPx48cjLS1N5zrmz58PT09P9T9fX1/TdoiIiIiIiByWpMHVvXv3oFQq4eXlpfG5l5cXkpKSDFrH559/jsaNG2sEaIMGDcL69esRGBiIhQsX4tixYxg8eDCUSqXWdUybNg2ZmZnqf3FxcabvFImChcdEREREZGtcpE6AORYsWICtW7ciKCgI7u7u6s9Hjhyp/v+OHTuiU6dOaNmyJYKCgvDMM89UWo+bmxvc3NyskmYikh82BSMiIiIxSFpz1aBBAzg7OyM5OVnj8+TkZHh7e+v97ZIlS7BgwQIcPHgQnTp10rtsixYt0KBBA9y6dcvsNBPRQweuJGHkz8FIzMyXOilERJIqUaqwITgat1KypU4KEUlI0uDK1dUV3bt31xiMomxwij59+uj83aJFizBv3jzs378fPXr0qHI7d+/eRVpaGnx8fERJNxGVenfDOZy5k44Zuy5LnRQiIkltOBODGbuvwH/pcamTQkQSkny0wMmTJ2PNmjVYt24drl27hvHjxyM3Nxdjx44FAIwePRrTpk1TL79w4ULMmDEDv/32G/z8/JCUlISkpCTk5OQAAHJycvDZZ5/hzJkziI6ORmBgIIYOHYpWrVohICBAkn0ksnf384qlTgIRkaTC4zKkTgIRyYDkfa5GjBiB1NRUzJw5E0lJSejSpQv279+vHuQiNjYWTk4PY8BVq1ahqKgIr7zyisZ6Zs2ahdmzZ8PZ2RkXL17EunXrkJGRgcaNG2PgwIGYN28e+1UREREREZHFSB5cAcDEiRMxceJErd8FBQVp/B0dHa13XdWrV8eBAwdESpn9Ycd9IiIiIiLLkLxZIBERERERkT1gcEWiORudjpTsAqmTQURkMkGQOgVERGTLZNEskGxf8O00jFpzBgAQvWCIxKkhIiIiIrI+1lyRKE7eSpU6CURkK2ykdkjBTqpERGQkBldUiSAISMsplDoZRFajADPRREREZD4GV1TJh1vD0f2rwzhxk7VRJB8CO8MQERGRzDG4okr+iUgAAKwKui1xSoiIiIiIbAeDKyIiIi1YW0pERMZicEVERNYl4y5uHMOCiIjMweDKRuQWluCVVafx83E21SMiIiIikiMGVzZi45kYhMXcxzd7r0udFKtgYxwiIuOoVALG/h6K6bsuSZ0Uh8RWpEQEMLiyGfnFSqmTQEREMnYpPhNHI1Ox8Uys1EkhInJYDK7IJgmCgJi0XHY4lwlbPw8C60qti4fbIkpUPLBERFJjcEWyVFWf8q/+vYb+i4Ow+tgdq6SHiIhIHw6GQkQAgyuyUb+ejAIALNzvGH3Q5E5hYq5i9t9XsPRgpMipITKdjVfCEhGRxBhcEZEkYtJysfZ0NJYfuQWVxM2ZFHIeG5yIiIhsBoMrkq3CEiXiM/KlTgZZSGGJSv3/bE5DRERE9oDBFcnW4O9PoO+CI7gcnyl1UoiIiIiIqsTgysEY0/zJmNoES/RTuJOaCwD491Ki+CsnIqqCqX0JiYjIcTG4IiIiIiIiEgGDKxIFC3iJiIjIGAkZ+Rj5czAOXEmSOilEomFwRTpxSGIiIiKylBm7LuPMnXS8u+Gc1EkhEg2DKyIiIjvAFgRka9LziqROApHoGFwRkU24npQtdRKIiIiI9GJwRUQ24cTNe1IngRyMwLbRZARzL5e8ohJxEkI2p6BYiX2XEpFdUCx1UkgEDK6IdGATG8sqf3iZhyW5sOX7nveR7dp/ORHtZx7Aj0G3pE4KSWDW7isYv+k8xm88L3VSSAQMrkgU9vhSt8d9Iu1sOUNNRPJ1KyUH1xKzqlzusx0XAQCL9kdaOkkkQ9vC4gAAJ2+xhYY9YHBFREREJDJBEOC/9BgGf38CmfmO3dwrKDIFs/++gsISpdRJIbI4F6kTQPaBJf9EZA9YY02mqvgeVJW7lu7lFMKzejXrJkhGxvx+FgDQpE51jHuqhcSpIbIs1lyRTlIGTMzf2BZTOv7zHDsugWefZCImLRcnOViO1cRn5EudBCKLY3BFREQASoPksb+HYtTPZzhSHjmE/ouD8MavIQiPy5A6KURkJxhcEZHZFGwXahdyi5Q4GpmK4DtpSMwskDo5RFZz8W6G1EkgIjvB4IpkiVl1Ivul4B1OROC7nuwTgysicnhsAUfa2FqNrI0ll8zw8bZwjPgpGCoVH15EcsPgisjOpOcWYebuy7gcnyl1UvTSmERYslTYHvaFIrI/xt7Vf12IR0hUOi4nyPs5T+SIGFwR2Zkv/7qE9cExeP6Hk1InhUR2NSELXecdwrrT0VInhYhkgGUtRPLD4MrRGNFshA9t2xSZlC11EmyOrTSnmvJnBDLyijHr7ytSJ4WIyGzMZpA9YnBFRGZjUzUyBue5IiIie8XgioiIKmH4Q0REZDwGV0RkNlsbVY2041kkMh0r8IkIYHBFZJb8IiWWB97EtcQsqZNCRA6OmXt5YXNpIsfE4Ip0YmVE1b4PvImlh25g8PcnrLK9lKwCHLqazLlNHJS1J98tVqpQUKy06jblhJljIut545cQZBcUS50MIrMxuCIyg7Xnknpq8VGMWx+GP87ftep2yTE9tego2s3cj/wixw2wiExlTHNplmUCJ2/dw8/H70idDCKzMbginRIyCvD22rM4ffue1EmxGrmXVBcUqwAAx26kSpwS87FmVP4SMwsgCMCNZA7vbwt4T9kueb95LKfiJZtdUCJJOuyRIAiyz9PYKwZXpFPUvVwEXk/Bf9eESJ0USTCjYlnln/l8AcgDzwI5KjEe93xnkFwIgoCRP5/B6N9C+X6VgIvUCSDr4rPfcIY8jzhfTyk+vK3DXjJv1u47RlQVPsHIniRlFSAkKh0AkFNYglru1SROkWORRc3VypUr4efnB3d3d/Tu3RuhoaE6l12zZg2efPJJ1K1bF3Xr1oW/v3+l5QVBwMyZM+Hj44Pq1avD398fN2/etPRu2AS+QMgWKFUC9l1KRHJWgdRJcSjWCnlYKEEkDnspcCFxlS/v5FQp1id5cLVt2zZMnjwZs2bNwvnz59G5c2cEBAQgJSVF6/JBQUEYNWoUjh49iuDgYPj6+mLgwIGIj49XL7No0SIsX74cq1evRkhICGrUqIGAgAAUFDCjRmQLNp6JwfhN5/GfJUFW2R5fPaQNMyVERGQsyYOrpUuXYty4cRg7dizat2+P1atXw8PDA7/99pvW5Tdt2oQJEyagS5cuaNu2LX755ReoVCoEBgYCKK21WrZsGaZPn46hQ4eiU6dOWL9+PRISErBr1y4r7hmZg+XatkXsTGhQZGnhSi5HqSMi0oktsonkR9LgqqioCOfOnYO/v7/6MycnJ/j7+yM4ONigdeTl5aG4uBj16tUDAERFRSEpKUljnZ6enujdu7fOdRYWFiIrK0vjH9mu/CIl/rpwF/dziyy+LclebHyhkhXxciMiIjKMpMHVvXv3oFQq4eXlpfG5l5cXkpKSDFrH559/jsaNG6uDqbLfGbPO+fPnw9PTU/3P19fX2F0hGZm75wo+3haBN351zFEOiYhIPFkFxfj2YCRuckoCIjKA5M0CzbFgwQJs3boVf/31F9zd3U1ez7Rp05CZman+FxcXJ2IqHYOcmibsiUgEAFxJYA0kERGZZ94/V/HDkVt49rvjUifF4g5dTcas3ZdRrFRZZXsyyjrYFR5XaUk6FHuDBg3g7OyM5ORkjc+Tk5Ph7e2t97dLlizBggULcPjwYXTq1En9ednvkpOT4ePjo7HOLl26aF2Xm5sb3NzcTNwLsgR2I3dMJ26mIvh2GmLS86ROisPj8PpEpcLjMiy+Dbncb+PWhwEAWnvVwhuPN5M4NSQG5qesT9KaK1dXV3Tv3l09GAUA9eAUffr00fm7RYsWYd68edi/fz969Oih8V3z5s3h7e2tsc6srCyEhIToXSeZx1EH1ZLJ+9Amabtm/vdrKH4Muo07qblWTQtPYylHvY+JxFDxfSCXgMkUnAaDyHSSTyI8efJkvPnmm+jRowd69eqFZcuWITc3F2PHjgUAjB49Gk2aNMH8+fMBAAsXLsTMmTOxefNm+Pn5qftR1axZEzVr1oRCocCkSZPw1VdfoXXr1mjevDlmzJiBxo0bY9iwYVLtpmww70SWYG4mwnazINYl1f0r9nYFAQi+nYbEzHy83O0RkdfuuPh8J1vDa5bskeTB1YgRI5CamoqZM2ciKSkJXbp0wf79+9UDUsTGxsLJ6WEF26pVq1BUVIRXXnlFYz2zZs3C7NmzAQBTpkxBbm4u3nnnHWRkZKBfv37Yv3+/Wf2yiLSRrKSfbySycaPWnAEAtPOpjXY+tSVOjX1gIYV88ZFN5DgkD64AYOLEiZg4caLW74KCgjT+jo6OrnJ9CoUCc+fOxdy5c0VIHRFVhZOt2jdLZtoTM/NlFVwxQCFL4HVF5DhserRAIofFNzVZgLW6iNhKLG7LfWbIOFJfkqYWUNnKvUTkSBhcEZmBeS/TyenY2Uz+xIo5KUetjXTMvSYie8KCIWkxuCJR8D4msi98OROVctByBiIyEYMrsjvMEhKZhplIshZBEDBx83lM23lR6qQAEOe9wfuH5MJRWx7IBYMrEgXvY02xaXmISbPuXE1ERLYiNj0Pey4mYktoHIqVKqmTIznWFBPZDwZXROUY+34TtJR3FpWo8NTio+i/OAgFxUqRUiZvpmQMGJDbDkfN+LH013JKVA+vqR8Cb0qYEiIicTG4ItLB1HxVftHDgCorv1ik1BDZD0eJ1abtvIiZuy/r/L6oRIXNIbGITcuzYqrkZ/mRW8grKpE6GTrJ+Xq1VNqk2mdHLcgh+8LgisgGaasxs2V8n5K9ScoswJbQOKwPjkFuofbAYc2JO/jir0t4avFRUbZpy/VsKj4DHBJrhy2DQaq0GFyR3RHrUc1nk+H4grRvPL/GK1E97Eek61Fy5k6adRJDkuArhMRQolTh1dWnZTP4S1X2X07C8sCbDh3gMbgiMoMDPztIBHJ++VgraTI+BEQAHLN/qLX2Wc7PQLkIjUrH2ej72BIap3e5KwmZWHHkZqW+3ta+ft/beA5LD93A6duOW3jkInUCyD6I/Xzk45bsXfS9XLyyOhjjnmyOd/u3NOg3ln5HKmy6YRmRtBgnmI+15JUpDbywhiw/Wbq8ChjevYklk2SQ1OxCqZMgGdZcEYnMGv2hKmaCC4qVSMjIt/h27ZUUL/Sv/r2GezmFmL/vutW3LSfMS5Gtu5Gcjdd+CkZIlPVL6nn/UEVXEjKlToLDY3BFopDLAz4sOh3ZOjqPW0KV+22l4/L0kiA8seAIbiZnW2eDZDZbag5jTlq1/VYuzwtbcfR6Ct5ZH4Z7OY5bEixnb687i9CodCRn6T4/FS/57AKOJEtkrxhckV15ZXWwVbcnl/xxYmYBAODwtRRJtm9LgQLpJnata1h0Onp+fRj/RCSIul5LssSVbO79MXbtWRy8moyv9lzVvx2ztuLYzIn3U/QEVWXKn5vvD99Ex9kHsTs8Xuv3jozvErIHDK5IlliwrZ99DMXOs2wsa9b4iLGtt9aexb2cInyw5YLG546QfzKkqamxzVFTHLgPgz357vANAMD0v3TPgUZkDkd4xsoZgysiIrIIJScv0oul9A85+rFwlKImRz/PjsQ+CoFNw+DKwdjDSDw5VuxTRYYx7bpy3AevLWAeiCzJlt5EHEWTiIzB4MqBnYtJx78XE6VOhtF+OXFH6+dSlIhVuUVmUA0idQkXS1NLGZOJVKmESvOp2BteF5ZjT0fWnvbF0uyhgFfuFAoOGiQ1BlcObPiqYLy/+TxuiDDCnDXzIFn52muuTty8Z71EEBlBpRJw+vY9ZObb5ghh2jJEQ1eewmOzDog26tnd+3lmNSPcdSEeG8/EiJIWc6lUAv69aP1BPJifMp2+K0/qwh8pWOudzmCL7BGDK8Ld+3lSJ0EUsuzszfcGAdgeFof/rgnBiytOmrUeqS4nbTU4l+IzoVQJCLmTbtA64vXMw3bgShL6LTyKdzeEmZy+SdvCMX3XZSQ9GDlTStvC4vDN3ofzlzEDWTUeI/skCAJm7OLAHeaQslmqSiUg5E4actkdwygMrkgUfC8S6bbnQfPbmDTzCjJsufy874IjOr8ra+orxlQCOYXi1Q6amuE/eUuzFp3NC8lRhcdlYIMRNcq8U8wn5uPmt1NRGPHzGfz3lxCjf+vIfRUZXJHDuZmcjZNsQigqZh5JG9ZG6MfjY9/4WATyiuy7X6Y1mNIsVaxrb0fYXQBARFyGOCt0EAyuyOaY2/792e+O441fQ0Tpa1ZVUpYH3rTZfjaOROpM7qGryZJuX65UKgGL9l+X7fHZdykR28/GWW17zKzLlAjnRW6nlnE/mcsR+yqWYXBFDutmco7Ft7HxTCymW7G9uW29EG0qsRY1br1hfY3s5YgZ+sr991Iifgy6bfDxsbbxm85jyp8XkaCnPxkZRs613xZr3mQvN7SIeEjMVzEfYK3mecdvpOJ2quXzVbbAReoEEFmaIAjYcCYG7X1qo4dfPf3LirI9zb/Dog3r8E/2y7aCXvmQw+AUhsgqKEZjVDf6d3IOKIiIDHU5PhOjfwuVOhmyweDKwThiHu9oZApm7r4CAIheMMTg3xmUIZbZAZUqryZ1szqyLLEvK14tpVQMruyDARc0r3ntWMAgb4Y27buamGXhlNgWNgt0MJZ6jMn5+XgnNddyK5fxfhMZS6o28vY+qpSuo3rqVppR62EZhuVIcmgd8P3BYIocAYMrOxEalY5wjuYiKr4DrIfH2jCOUEP43oZzZv3e3GtJjMxfUYmq0mcHriRh/MZzHODmAXu/kpUqAaduPxyVlo84w/A4kT1gs0A7cDUhC6/9FAwAuPPNc3ByMu61ZS8ZW5aIEdm+/VeScD+3CHVruNrsaFPaRjd890HQ6O3pjlkvPGbtJJElablMfz8Vha/+vWb9tJDdsdWa/YpZsrj0PDSs5Qb3as7SJMiKWHNl4+6k5uC55SfUf0uVFRG7QN02s1SlrFG5wDiSrEnfJc1L0Tgp2YVSJ4GsYOf5eKmTYBMcoTaegCsJmXhy0VE88+0xqZNiFQyubNyR6ylmr8Nenm1SPKS1law7YuBjWq2h8b8pUVZubkW27Vqi+fPN2dQ9Z8G0RiaJMHefldjSKbPGq0Vur2G53VOCICAlyzZGD6VS5e+b/ZeTAADxDjJtBYMrIjJaTmEJVhy5afXtPjp9H46KUKBgDXfv5+GPc3dRLFJAeC0xC+di7ouyLjGZkiksn29bfCBSrKQAkGdhUfkkWbKp49Sdlyy2bkuTcw2GmIHGgStJ2rch3iZkRaxj98mOCPT6JhB7LyWKs0ISlXzvXmmwz5WNSszMR4nS9h7HB68kISYtD+OeaiF1UuyStfInX/97DVtCY62zsXJUQumEu7e+ec7q2zbWU4uOQiUA93OLRFnf4O9PVL2Qhdjek0a+yjKbptT2yq02QUyO0GdWqRLUfe/MYYljlZlXjC1nLf9MNzWILmtmuTzwJp7r6CNmkohEx+DKBqlUAvrMPwIA+Nj/UY3vSh+68i1DeOfBi6W7X110a1pX53J6+3iI9F6x1c7yVbHGXp2vUINi2gtTPtepJVKienAiTpcbMczRmHpcz8XcxxOt6hu0bHJWAXIKS6pcLjEzHx7VXODpUc3EVJmu/D1Z9vzacCbG6umwJkEQEHE3Ey0a1kBtd+sfc2OZ89zU9i7R9kisNK+ZCA8esQY7mPJnBA5cqTwQi7lkXCFJVuQAZScaGFzZoGLVw2ZGKdnGtUG21HPO2BsnJct2OnULgoD8YiU8XCvfLo72wCDTyLnJkxz93/owdPatgyEdvatctvc3gVUuk5ZTqC6QMmYi8YJipcHLGqosI74q6Lbo65aTA1eS8d7Gc3ikbnWc/Pw/UieHqnD8hnmFQIIgiPKcu56UjSsJmXissafZ65KDDWdicPluJua/3NHokZzL2EJBsPxTaF3sc0UOy5ASv9O37qH5tL1oP/MAbqXkWCFVhtH3DtP2VfDtNHz+x0W7mGOnRMXHuLUFSdDPLcLEefu0FXhcN3Ggh70Xxe/fYe0Cmbyiqmv1LOHfB31j7t53jA7s1pJdUIzgO8ZNPm0KY+Kk+Xuvodc3gUgVYSTM0Kh0DFl+0qDaaFswY9dlbAuLQ9AN6z1D5TJ0uyMXPpsUXMXFxeHu3bvqv0NDQzFp0iT8/PPPoiWMDGO7BeKld11OYQmCb6dBKdMM839/CVH//7rT0Qb9xhp7ouuh9a+OzOCoNWewLSwOiw9ct2CqyF4tP3LLshuQ6XPMEveyNZ90G87EoP3MA9h+Ns6KW7WO5KwCTNt5EVcTsqROilW9ujoY/13z8L0kh1qNn47fQWp2IX4+Ll5tbEaeOH1VdQmNSrfq6IPZBbYZLIoVINluXtU0JgVX//3vf3H06FEAQFJSEp599lmEhobiyy+/xNy5c0VNIOknl5IBU2+c19ecwag1Z/D7qSiz0+AIHaKrcik+U+/3ceksRbYVJUoVwuMyJBt+3pK3k4O9ZzWY85wy9jk7Y9dlAMCUPy+avE25mrQ1HFtC4zTmebSUqo67tpoCS2UmTa2FtQZDmgWWv/6vJGRKMgLqmTtpeO2nYPQyoEmxHFiiJup6UpaorXGqSqGjZc9MCq4uX76MXr16AQC2b9+ODh064PTp09i0aRPWrl0rZvrISCbNNvTgR/9eTMTk7eFiJqdKEXdLg4E/zt3V+Hzy9girpqNMicq4jKzS0Z4YOth6YGvp1Jvyapy75yqGrTyFuXuuip4esn+2fk9WJTJZPkGGobVHlZ4DMjpFUtSADVl+0urbBIDTt41rVlmsVOHo9RRkFYjTrP5+bhGKSqSbszGroBiDlp2A/9JjUMm01ZCtMym4Ki4uhpubGwDg8OHDePHFFwEAbdu2RWIi5yCwNEu9M9/ffN5qs8pXtQ9Stbc+cs24dtEXYjMskxCUluo9vSQI+6w0r8fFuxnYczHBKtuiqq0PjtH4rz0x9BFmj/GBI+ZlTt2qPFhC4DXxR6Yzh9h1AxYYGFCWdodbJs8gp3t/eeBNjF17FqN/DTV7XXfv56HrvEMIWHZchJRpV1WgXL5vnCX7MDtaU8DyTAquHnvsMaxevRonTpzAoUOHMGjQIABAQkIC6tc3bPhcMk5uuWDjRrkSu4oXryNcyxdM7OhuiIIS80cHE+scjN94HlH3cjF+03mR1qjfiytOYeLmCwi34PF1VDLKJ+hlSoZGlH2zlQNkIlOOq1Il4NiNVIv3PbGG18v1XS1zPibD+gmRMTkFE4YqLFHio63h6r8FQcCd1BwsPRhpF9dtmbKWNWK8G/stLO1SE3Uv1+x1iaHHV4dwLdGx+i1ag0nB1cKFC/HTTz9hwIABGDVqFDp37gwA+Pvvv9XNBUlca0487JP04opToq7blkoXCoqVJo8iZmvyLTIM9EN3UnPw3zVncLpCqfKdVPmMimirskVqPiIlW3ouiEkuedwNwdF487dQvLDiYdMpW8qAG3v52NCuGcTW7x9DrjVtA1EN/v4Elh+5hS8f9Pejh6w1Wq8CCoOfFVkFJfh0h2W6YdjS80psJs1zNWDAANy7dw9ZWVmoW/fhRLDvvPMOPDw8REscGc8a17JSJeCfiAR0b1YXvvVKz7e1bqJcPc0FOZeQcd7ffAHXErNw+nZalXP/3EnNQbP6NeD8YJ6Ois0OTDn29nq6YtJy0X9xkMZndrqrAETaNwsfIDke/6qemWVDmctlEJobydnIyCtGr+b1pE6KRV2OLx1k4X+PNzN5XiJttJ5uPavPLSxBfEY+HvWqJc72q5iHSqx3eOGDvkQVJ5oniN6/KSEjH5FJ2RjQpqFZ+R9HDoIsxaSaq/z8fBQWFqoDq5iYGCxbtgyRkZFo1KiRqAkkcYmRod0UEoNJ28Lx5KKj5q/Mhhja4deY51RmXjHiM4zPPJn6MCwoelgblmrgBNTbzsbiP98ewwdbdDdPPBdz36Y7xoqZ+f7TSv0WSVoJ5e7bqu7H8s9dOQZ6hhj43XG89lOwxn7bo+d/OIlZf1/BLgv1JTLUoO+PY+B3xyu1LDDFwv3X0VukeaiMxYy75Tyx4AjGrj2LQ1el77/Iwm1NJgVXQ4cOxfr16wEAGRkZ6N27N7799lsMGzYMq1atEjWBpJ+xDy4xHnSnb5k/gaEcnrdVBUtiPyuORqbg/9adRUq5oKbz3IPou+CIxmeWFBqdjl0X9Gca9l9OQmhUuvrvVUGlc5fsvZSk93fWmNiSrIeZIt2Cb6fhiQVHTPqtrR/W2PQ8g5araj8r1X6bmB5LMWbIc233irnDZ5fVWP5bxYBGhmxnVdBtpGQX4qdj4s1D5UjEujYtFX+cuZOu93u532v2yKTg6vz583jyyScBAH/88Qe8vLwQExOD9evXY/ny5aImkGyDIxRamPuyHPv7WRy+loI5f1ceWvuKFSfCnLQtXO/3B68m47WfgvUuo+1YFBo5GIgjZd4d4f7QSib7Lfaltjk0VuNvU86vw14TpJXU14Olti/1fpkrNi0PCZnmF35a8n2nraDY2se9qqkf5DDZtTWZFFzl5eWhVq3SdsAHDx7Eyy+/DCcnJzz++OOIibG/YYPJPlUVLFnqYWhoLZUjBR+OtK/keMpf3zae1xSNJSZGtUXpuUU226Ra3zmU9TPdiMQN/t5yQ6YbSsoAVc6nUc5MCq5atWqFXbt2IS4uDgcOHMDAgQMBACkpKahdu7ZR61q5ciX8/Pzg7u6O3r17IzRU9zwCV65cwfDhw+Hn5weFQoFly5ZVWmb27NlQKBQa/9q2bWtUmmxJxZvOGg80Q2705KwCXI7PNHidglA6UV/V29b3MK+882ej9VeXW8vywJuirk/fOeDDUGKyzlXIE7PZtsPQy9vS59QeJknOLihBt3mHMG59mNRJ4WNLh9wi8UbtNbVAoapzU2lONYVx59NSwZtmoZJjPeVNCq5mzpyJTz/9FH5+fujVqxf69OkDoLQWq2vXrgavZ9u2bZg8eTJmzZqF8+fPo3PnzggICEBKivaJXPPy8tCiRQssWLAA3t7eOtf72GOPITExUf3v5ElpZgG3BjEeiJZ4qPb+JhDP/3ASt1IMa7eelFWAdjP2i56Obw9Gir5OUyw9dEPqJGhhnYfdzeRsi3e4vZaYhZVHb6HAAsPXU6n3N53Hx1U0KbWkraGx2Hn+rmTbt2cHriRh0tYLyCuSZvJ2ORD7NWhMhjXwuu7J6xnzWICtt1Uk2TNpKPZXXnkF/fr1Q2JionqOKwB45pln8NJLLxm8nqVLl2LcuHEYO3YsAGD16tX4999/8dtvv2Hq1KmVlu/Zsyd69uwJAFq/L+Pi4qI3+KqosLAQhYUPR9HJyuKEamI5H5uBVo2qHkrWWvM/WIPUJaqGvzaqTqcgCIhO0+zAfi7mPiKTKwfNunb72e9Km1X88V4f9PCzzDDOg78/AQDYdzkRKhXw/cguaC3SEMZisMVXecU28mUd6+cN64Cabia9OswydeclAMDznRrD1cW4ckFTj7/czpsxeUJjHkPvbjgHAGhazwOTB7YxMlXSsLfRyaR6bfx6MkproZQph5e1X7pZs8+RFOdB2/1Y/iP2uTKQt7c3unbtioSEBNy9W1qa2KtXL4Ob4BUVFeHcuXPw9/d/mBgnJ/j7+yM4WH9n+qrcvHkTjRs3RosWLfD6668jNjZW7/Lz58+Hp6en+p+vr69Z25eTvZcSsbeK0YbEIMWohXIjCILNtp3XRduM9MNXnTZpXVetMAv85fgsXE3MwsTNFyy+LWNouyrsLG9oFl13jbbPt4TGYnOI7me6fd2B1pViwFDdlsok2dt5s4V3XFGJCvP2XMXiA5Hq+anM8cvJqCqXic8owMzdlxF1L9fs7RHJlUnBlUqlwty5c+Hp6YlmzZqhWbNmqFOnDubNmweVyrAb9N69e1AqlfDy8tL43MvLC0lJ+od81qd3795Yu3Yt9u/fj1WrViEqKgpPPvkksrN1N0+bNm0aMjMz1f/i4uJM3r6c5BSWYMKm85iwSff8RAAzeeXp7aBbxev/vY3nMHDZcZSIFGDp7Vdl5ia+/OuSQcvlW7CZnSWvuxw9k03LhdwzX4amz9q7MevvK/jir0vIzLN8bbcl982U8y/3a8YYcn/vGNMCQaxnvinMCXZVVrigyqbyKHP4WjLWB8fg1dXmFaKbxcL7ra3/uCX7HEndWoYqM6ltx5dffolff/0VCxYsQN++fQEAJ0+exOzZs1FQUICvv/5a1EQaY/Dgwer/79SpE3r37o1mzZph+/btePvtt7X+xs3NDW5ubtZKokWVf9Dma+mIaYkX2q4L8fix3AM0Lcf6ExXqYs1nzoErpf2K5DKIhj6bQmLRoKar1MlwGDLPR9qk0qH/q0my7Yrnk3kb2yf2PVrxXauAacG6rT87TuqYBPmejPIJSpUABQAnJ/OP9vRdl7DxTCyOfjoAzRvUUH8uQLD9k0kGM6nmat26dfjll18wfvx4dOrUCZ06dcKECROwZs0arF271qB1NGjQAM7OzkhO1uzonpycbFR/qarUqVMHjz76KG7duiXaOuVEDiX0FedNGl9FTRlgmaYllm6Db2jJk1glVJbPsPFJT5qkyvzZ/ZVYftQskXc2q6BYliXX1qyZyswrxunb90Rtli2XPl1inllL9dnT5ULsffNXYgnlDkSJUoUBS47i+R9OinIfbTxT2mTZmhM2V0y1TC5dh2ZScJWenq61b1Xbtm2Rnm5Yqb2rqyu6d++OwMBA9WcqlQqBgYHq0QfFkJOTg9u3b8PHx0e0dcrJ7vAEq2+zqhs3NEr+NTdyw2ehxKx4AgRBwLcHI/WOECYHuu5za2bkTTkt5p5KQ3bPnCMg5uELuZOGTrMP4rM/Loq3Uhs05IcT+O+aEPzB0ST1snYMPm79Oetu0AQx6XmIS8/H1cQsWLp1pzWDHn0F2NYqOJBhmY/VmBRcde7cGStWrKj0+YoVK9CpUyeD1zN58mSsWbMG69atw7Vr1zB+/Hjk5uaqRw8cPXo0pk2bpl6+qKgI4eHhCA8PR1FREeLj4xEeHq5RK/Xpp5/i2LFjiI6OxunTp/HSSy/B2dkZo0aNMmVX7da609HW2VC5m8sWBnsQ45ljSq2csb/ItdKQyabUwqVkFeCpRUex8qh91hZXJadQfz+1YzdS8cMR+R+bpEzDJru2JNGeGCbe14b+rMrnhoXyMmXX0R/nTAsqBEFAZNLD/shi5rmsmbG6ez8fALDPCoM3keGKSjg1hhhYE2V7TOpztWjRIgwZMgSHDx9W1zIFBwcjLi4Oe/fuNXg9I0aMQGpqKmbOnImkpCR06dIF+/fvVw9yERsbCyenh/FfQkKCxjxaS5YswZIlS9C/f38EBQUBAO7evYtRo0YhLS0NDRs2RL9+/XDmzBk0bNjQlF21W7P+vmL1bR6NtGxJvRgl6rZS0nLipvZ27NrEpedVvZCIvg+8idj0PCw+EIn3n25l0G/saZjW305VHjGrfEmhISOyycEKCwbHotyrFlvY7J8ZRA4Zpo0hsZix67LUyQBgf7X3lfrlWWw78jxy5j7TLfouluBFb8lN2kK+xRbSKCaTgqv+/fvjxo0bWLlyJa5fvw4AePnll/HOO+/gq6++wpNPPmnwuiZOnIiJEydq/a4sYCrj5+dX5Ut569atBm/bHsn5AhZjLit5vkY0SX0O7lQY4rZsDpvKLJNQpY4aSls4d5Ygl74bUnDcPddNTpfDmuN3NP426Nklk3eMHPuZyV3E3QxR1yena9mWaRt8zJr0j0zM+8wUJs8E2bhx40qjAkZERODXX3/Fzz//bHbCyDSJmQUaI9QYQop7535ukVU7fBrKHl8Wt1JyRFsXH7TGEwTBJgMsa55qUZujibcqnYxNrqVGUD1vxIABiZn58PGsbpF0aFPVObXE9WXLTydr1N6n5RYZvOyKo7cw+olmaFTL3YIpsl0qlQCFAvg7IgHHb9zDguEdUc1Zd08bXfdDTFou+i8OMistFa+di3czkVuueTpf29Zn8iTCJE9PLwnCiZupAMQPFIqVKpyPvY8SpRF3qo40zNh9BfP3XRcnYVYiddO1lCzj+8Dczy1CkZY5N0qJe4FU9QCX9fPd0h2Zzfz91QTLT8BMlrPaQgVJeXpKvCte0mN+O2uRNIjF2FvQ0gUWZhckSVigEpeehx8CbyIjz/BgSpvF+yN1fpdfpMTBq8k6v7eGqwlZpj0bzTw3RSUqPLP0GN5eF4aPtobjz/N3sT1M//youi6nDcExZqVFm/iMfLy44mSlz7MKSlsPWevKLH+YbbB80SwMruyQvps1MbPA5JfGjF2X8fKPpyV/oFb00o+nTAo8bElqdiF6fRNY9YIVfLw9XPzEOLilByPx9JIgszMuxlh7unI/LkuTujAB0B+w29K7Wlv5hrl5d20TleoTmZxd5TKGZICkvyrsk5h9p1768TS+PXQDn/9p3iiS+gZO+nRHBD7ccsGs9ZujoFiJ55afwHPLT6DA2Mnuddx8huaNwmLSEXUvF0fKjfh634BaQVPPcJUFl1q+rzix9Y6wOHSafRCrj9222j1cPl2OVnvG4MqOabuYFx+INLnGaOtZ/SUzpqRHDBdiM7DACrVgUnYcjojLMOl3QZGp4ibEDLaUGdZn+ZFbiLqXi99OGh7wGHrpbzgTg2ErTxn0orY0Q+9XMW5rU54N9v6uLirRHzytOXFH7/eGkDKArhjIXU3IwvubzyOqQp9RY8SaMXhPfIbIBXQS5ibLJugNvp2m8bmYz+B/LTwy46aQGL3BTm65OT5zzZjv05RjoqtfsTUUK1XYcCYGd1KNa+5fNl2DvrySUiVUClRtsUm7HBjV5+rll1/W+31GRoY5aSEr+blCJ2Zz7p2dBs4rIggCPtkRYfqGqqBrMmUB9l1icjY6Hd61dbeJd1LA4nN3OCqlgReWoS+nvKIS9cht3wfeNDldtkChUFi5U5dlV2+JXalqVMmjMp8nTZuVR28ht7AEUwa1rfTeGfnzGQClQdbRTweYtP7MPNMHTVp97DbGD2ip/tsSmUqp30XWziibs7s/Hb+Dx5p44sXOjatctqCKggixGVOwVkYQxDn+v5+Kwjd7NQMksS6roStP4mpCFiJmDUQt92ql6zb0PSdSGuyFUcGVp6dnld+PHj3arASR+az5/Jy8vYqA6cF9KWUGf11wtEm/s+ZxNGVTVxIy8erqYL3LOCkUUBnwcCzR0cTI3gutLsdn4oMtF5BtRslnVQRBgFP5A6njdLSfeUD9/xVLD6XOlJnK0MyEXK8zY5KVkl2A7IIStGxY02LpsWUlShUWHyjtw/PG4810LmdOzZW5RJ22QqSL2px730YfG2rXErMMCq5e/vEUQr7wN2kbpuRNjkrYIiQs2vBBbIx1Ob60/1rInXT4t/ey2HYcgVHB1e+//26pdBCZTVdGrmyCSUNYZAQrC73hLt7NrHKZ0kx91Ql4fP4Rs9NTthVbCgTe23jOqOtDKidvGT6vmVh0nceKH5fddVkFxbibno/2jWuXW4d5F4PUQZcx2+/1dWmfyNAvnjFrPerfGP8TWdE3z1OhlWsayDTmNoW35DVc/n2fnGXkaJzlfnvwapLWdcqJ2CNvynMv7Qv7XNmhsgeiTJ8TNqPqTqTiRREWm2DSwGvgnoWGitbF2M74lmJ0R+hyTD79JtyX9y04eEbUvVycFiF4e3pxEJ5bfgJn7qRVuawlh/SX+rFnyMARhqgqoyf3QgyZJ89uGNNvztT7LjW7ED8fv22xKQVMYdYzpNxvU600qfuu8HirbEcqvN81MbiyQ3IY5UsK2h62xma0jAlI91w0rEOvlEGus5P1Nm7olg5dTUbrL/dha2is+jO5ZxStzZrXzNNLgvDfX0JwNSFL4x4yNg1lc+gcvCLeaKKiDXRh4vVl6PYtcb44p5zxLFdIZf4JljroN8fb687im73XMWHTeatu155ugRM3rd/6wFS6Dntceh7WnY42q1DSUTC4Irug62Gg79lsbBBacfkbIpVQ69ti5f8zjtgvdENedjrzIQ++eHdDGABgzQnrDy8uBYVCYXbmW9tx77vgCHZdEK809GqiYfPF2HImUR5su2WBXDK8lgg+M8wYEKMiY0+vrvdRVdeJRUexfbDqsiboIVHpBv9ULteJMcy5pkztV67t/E7eFo6hK0/p7AstNn3XUPnd8l96DLP+voJvD+qe/0zXbx0Ngys7ZOzD1pDniVyaA9hqhkQqsmpDbotvWxGIkQnUtob4jHxM2hZu9rqpVEZeET7ZHqExfLU1r1hBEDD1z4tYcaR0pEhZ3bsis9RIh8Vm9uX6ZEe4+v/L37dS1iJK+tiU8SNbrPtDrOB0c0hs1Qtpoe387rwQj4i4DJyLMXTwCuNOlKmHrqyvZLCWpt9VrVLGl5JFMLgig8zYfdm0Hyo0/iO5WykPa5uMfagaurw9P0SWH9E9RLih+21rx+enY7cxdMVJ9ez2Fd3LKcTaU1EiTyqs0POXdRg6oIU+Bo8WaOk9NGD13+y9hj/P38V7G8+ZuSnDshkVj+/Fu5nYejYOSw7eMGv7tmDunqs4a4FRz7ILS/BJVSPY6mH0wAg2xtiA5N9Lidgt075C1gh4q5prrrykLJHnSYMR71SZvlTlku+TAoMrO1T2/BTzhruZbNyEdWW+2HkJSpV8eoHN+eeqaOuy/IPDslswpfRq2WHbmH9JzBfv/H3XEXE3U+fcJm+vPYvZ/1ytohbJvHMpxehqpt61pvwuJk264bfLGDMBrSUqlfIrDb9f+TiKPXiBvkXC4zJwzYCmoroGw6k0WmCFbZk7kICuQOHPB3Mv7r+chJsWb7ptODm8A2+l5ODIdeP6RH60NVzjb2P625i7zweuJFW9kIUoVQJ6fHVIsu1bkhTBmKMFWgyuyKJKVAL2Wngmd0CcG1cQBNxKMS2ItDRt+2eNB6S5x3XM76HlViavx2tKtnEljboCnIgH/RGC9M59IoeslX5SNn1KyNR+LvQFE3Itra2avO4DTaVpy8grwrCVpzD4+xOVlih/TjaeiUGPrw5jqYF9MKwl+HYa3tt4Ds9+d9zkdZjb7Exmjzu1t9aGmfX7tjP240Ks5eZaKk/fnGdiNQvU9YxJyylEVoHl5j8so283DH3GVbWcOVN5SJX3sHUMruyYXB7uGfnidRLWJbeoBGtPR1f+woiHwPrgGPxw5JbZaTlrRKffMnJq9iXmevUHHOYrLFHi2I3K26jqxfvvxUT0+joQ93IsN8S5oWmRQnxGPv6JSIBShNm9K897Jb/9tbSqp20wfp3WuG50bSLFwFql6btKm4svN+C5ac3b4EpC1XMAWpq2cy6fNhzm+daOmq5a6j249Wwcnl4SZJmVG+lGkuk1uGIdHvu48g1n1CTCZJ9kmPcz2qlbVc+tA+gfDnX1sdtV/v5WSg68Pd1R081F54E7cVP8gELbg8kezpu55vxz1aSOxIaOdiQGQRBkd7L6LiidNFpXPzJAd6Zj14V4bA+Ls0SyJGVOJiunUNwS7l0X4hGfUXly6/KBqxiZFZZAG0baAhJz5nMSLxWmEOOoKVUCcqxQg2QJFWveLNU6wNh5Ks29nA39vSM/XlhzRQbJK+K8BhfvZsB/6TE8teio3uUqPlBMrx0QoVZBz8Pc2hMHV+V7E/pz6QqskrMK8PkfF/Hs0mP468Jdc5MGoHJG1MjxmURJg9hO39ZdKKFr/2buvoLL8br74ohZOm9KLZiY+RdDt/7q6mD96zFyN6QbBdKRs0MkR6/9FIzOcw8atOyF2Pv4bEeEUf35yt+btnr1B4o8+mZVQaC2r2VWfig5Bld26PTtNCw7fAMqEXMZJSrrd6gXhYg3fFnfmvRc45qSGdLERq4PdWNKbM0tlfvusHhNTUpUAraFxeFmSg4+3mb66GFS2RJq2rC+5pDLXGoGb1fEDWfmF4s2h09ekW2WspuCGSrdjD02ugsSzDjIOn760o+n8E9EgunrtSLDhyMHXvrxNHacu4sv/rpk8G/KH/etZ+PKf2EVl+J1N2G1pWakrAXXxODKDmXmF2PZ4Zv4O9w2Hp6O5mz0fUkHDzCGtdJpG0fDsYiRtzB78mSRBrSoqgZMV62pKdflOxvMG8pdWpbLUWpkXC1MjGZ81n5GF1tiwlgdu3AhNgMfbLkg/vZk4k6qaQNTzdhl4pQzZjhzx7g+2kUmXCcsBLE+Bld2LM6IoYXtlhnvx6uJpnWKNuQ5pnXwDQuwZl+Bsm2Zssk1x++InBrrSDZgbhOFQmH1l9u3ByOx1ZDar3L3R8UkGlOTo3P1Ugz5q+VYV1UCnJZr3T4L8qVl+HeRTqLYGVdrBD8lShX2X05CmhWaUJeIMLiMtRhSo8K5jy3D0oNEVWJAE8CsgmKsOWHaO/xcTDpeXHES52LMf9/ICYMrO2ZoxtrS7ygFzH8RqlQCMvKKRB2JrKoXxOd/Gt60oDxDjruY823Zg+8DbWP+rPIUAIauOFXlctYuAb94NwM/HLmFqTtNu37JsuQcmG0JjcOUPyL0PmUFPQF5eeesNFy3Jf12KgrvbTxn8OiJ5V2IzRApFfqfH3K+niz55NO72yJsWA6jnloqDRXXa+w1VPGVNmPXZVw3cUTC4auCcfFuJoav0t9v1dYwuLJjYmbqzFmVGKn4v/Vh6DL3EC7GZ4iwNvtglXmu5PzmlpgAIMmAmitryzRi6oM7FUezEjktYgz1bg0W6O2ik3nPUssfz+1hdw3eir7l4tIrj3ZoLWK9+w5cMW7CXX3k+CS1kdbpWok3RLjmmr786xJKLNFE0wS20udK3yjM2hjS4sPWMbiyY2LellLnsY88GA1nfXCMaOusVHpj9O+1s1ZNRWFJ1SM4WrPWpGxbtvzCtgoZHZ9riQ9H/bNEssQeolwMs3Zfxq0UzVJWGZ0S0Vh6n4wJ4q3tq3+vmb0OWylYMud5++kO2xvsxxDmXPubQmKxi/3VRVOsVGHxges4c+fhyLS9vwmUMEXWweCKoFQJKCqRR0mNlMTKjJhSWG/Ka9zWmhaW7aOjBV+2kkmzNaaW6q4LjsHzP5wUOTWOQwBwPvY+Os85KOm9zPvKfP9eSjTpd/JoMmc5lupjJ5dXX1XPzow88QpONp6Jwcqjt0VsJmsbOIkw4Z0N5+Dh6ix1MhzamN/PAgBe6f4IFg3vBCcn6V9extoeFodfT0ahjkc1qZNiMSoxmrnZ3qkVncHNzkw43Ib8pqBYszDJ0FOSV1SCJxceRZqR0zFobEuk8y9GrbQhq9A2B/bqoKonXKfKTDtlpl8wcsnMW5slW2xYtcb2wW5Y+5Ux5c+LeK2nrygbj67Q9NxRsOaKAFQ9SbBcahvEfsgkZxXg1dWn8bcJc37oyiSZk3n649xdHL4mXjt/cyRnFRi1L4evpSAkKl1nPwUxL6HZf1/BqVvGtfM2lwABm0IeNks19J6wlWH3TVVx76y1u8aUnn+zt3Izsb8jEqoc/rpSUHHsjlmBlVi++OsSzluwJLj8br+zIcxi25Gabdyb8kujobXGtlCOZGwt3NAVtl/rbdQ+G3CqbeE8WxuDK7I4MW88sV8zc/+5irPR9/GhCXN+mPtePnAlSevnablFsujP0PubQNkE1RWtPR2N138Jsfp2/zgfr/7/7WGGz9tjzZePXM+ZKcSq5bkcn1Xpsw+3XMDPVUwBsOTgDQz87hgi4jIAADeTTRsRq+Ju5BcpTRoMpSxTtDlEnEmmDTm+h6+loKC46v6d9kYQBFm/uwzBTK/pdN0b0Wmc4oaqxuDKjsnpwSrX/J4lghhDj/u7OiYbnbbzEjrPOYh7OQ9LyE0ddU1O/RKkTElZCfXl+Ex1Rtlc6TKowZCDO6k5GveRjC65Km0Pi8PJm/f0Pp9uJOfgldWnzdpOxZrxr/4Vv7+kKbUwhv7Els4pPSTX9y5Zl9hzLVZ8bvA6q4x9ruyYPY0WaCnmDHVqzWMyb49tDV4hN59sj8DCVzqpBzJoVMvNtBWZkIG1doBr7Xt1/r7r+PbgDYus+58I3R3u84pKsOtCvM7vDRGTloc3fg1BLTf9r8JipXlP08IKAwbtv6y91roqAgR8ezBS63cXRCo0AIAPqqjJt9f3QUXMNJpHquNnzHa15QHkct6FCv+1NaYU+BQrVajmbPv1Pra/B2QVcmlqdDWhcvMeffQFTxWbuoiVX7DEoYrPMG3OGHOHwhY7E5WUKc38FjsvxGtkcPXVWO69lIhfTJxtXu4KS5RYU0VTOFMUleu7VPas+OPcXZ3LG3pZlR8qvqIZuy9j0rZwA9ekX7aB94kY98Ohq8km99s6cycdPxy5pfU7MZvuVZwQVC7P//KsNaCHrRAEASF30pBqwoTHlmDrx9aeyg/E7ltY8Tko1rH6IfAmWn+5D+ftYAJy1lyRQcy5NcW8rU0NMrT54chNPFLXQ7T1lbGnh7LYHp8v//ktJmw6DwDo07I+Hmvsafb6BEGQTfPMX05EYfEB7TUfYpv650WLrv/UrTStn8vkUBstSwb9LA1hG4NA2A9DD3fQjVSM/f0sqjnb6A0gImMuUW2DO8jlCIra56/cMTEmODLnbi9tjmj4Xnx7qLQFxOy/r+Dvif3M2LL0WHMlc7mFJXhr7VmpkyGb0jAxRafladRsGfsQ0TXiDrMe9kFbnypRzq2EF8jFuxm6v+SFK6mtZw0fIMXeyWHeRbkUiFSlLJnHIlMBmN+E1R6Y09zfkqQqlxD7WjZmPxy1MIbBlcz9ciIKR66nSJ0Ms8zYdRmfm1GKLcrcQiLbEBwtdRLIDIa8a2zxnbA1NFajRsdv6r9YuP+6+m8nPTsedz8PaTnWGaTDkofWkudNDpOnSmHjmZiqFxKRmHlBcwYtksPZtpEYz2bJNRADHj4n49LNG6GwtPXEw7/P3NFe60/iYXAlc3IYklsMO8+b3vHckiVxpvYBmrH7isgpkSe+1zVVdTy0zYFSqdTQQgc1PC4DU3dewupjmhO8rio34au+4OqHI7fw9JIgyySOrE5bkGlqv6yv/q08T5ityNczh6Mpb5aYNMtPipogYvN3e6Uv6DS3gOVifCbO3Ek3byUiKSpRYerOS6Ku87M/xGuy/UPgTVnM/Sc3DK7smC2WvGujVFmuicjtVNNflDonETZ5jSRrQtWZsYi7mVZJijbx9w3IkPHitF8VLs7/rjlTaZF2M/fjvpkZIWu8Vgx5d1m62Z6u5kx7LuoewVIsTyw4YvRv5FjDZYkkXU/KrnLy76qUP7PaaqP/vZiIqTst22fUEFP+uIhHp++TNA1VncOyflLaOGqTQIDBFdmAEis1CzT2RaBrElB7epzY1b5I9KC31nb1NW+5nZqDBfuuIzPPejXhSgd+scpBWEzlEbcEATh41bRh4G2VXJp9mRpo8DaqbPL2CORbeGJrOcSqYg3gZU5BhBSXnz30G+RogSR7KjPeLpZ8MW0OibXcykly2i6d2yk5Vk+HGIYsP4GCYusOEsBMoXFsqZS3qr5ns/8Wv9n02eh0TP3zIua82AH9Wjcw6rcFxUq4uThJPkjFUTP6T397MFLSmnFTWOqK/iciATXdnE3+ffmr4NSte+YnyEB9TaiRFIsC0gRKptxz+qbgsBWsuZI5uZS6SUlpRs2VttJbMpyYWRGpm62Uf8gbko/9/nCF5g4KIFdP/w2925a4HNTagRVJwMBLzBoNAdaejhZ9nSN/PoPbqbl449cQo37XZ/4RtJ2xH+9sOFfpO2sHtOMfTPNgih+O3MLxG6mVPte3C5baPzmUA1zRM+elMemLTM6ueiGRVJx3ctCy41bbNlkXgyuSvfOxGVInwShyaE5AlS3cd73qhcqpdN2ZmKGQurScRMRTaTU5hSUawYE5hWxA6eTNxvj3YiIL5xyYoROLm6PiZN2WsqxiQaENuJ6UhewC2x3QjcGVzEld4i0Ht2y0KRZpkrq0c4OVh5PWSeTjUFSiMjvjaU9stbafQfhD4XEZ6DDrAD7doXtQAVNqZSqOwlexJkFjWRNHkhUbLwvtzHmfGPJTqd9XYipWClapsdbG1NrTQctOYMDiIHETY0UMruwYH8rGEevZY0fPZLtl6L2xaL9xtV3aWLrpUc+vD+PpJUF2lRkw18qjtzgXnQ5mXydWuM5WHr0FAPjz/F1R11txFL73NxvfTM/aTQkN3Zz8CmL5QLIZel6I5lzvCoUC64JNL9S05SHeGVzZMWa2jJMoUkmlPR13MQN0Wwz2fyw3R9RPx++YvB5L7ntmfjFizZxk0p4kZORj8YFIzNh9RfQJyC15CZudaTfw57ZQs2fJFM7f93DOrvITblua/I+64eT+LK/qXpJ58q0uJavAIoUGaTmFoq/TVjC4kjlzXoS28BIlkoKkpbwW2rTcMzzWklv4cNARHhPbZMkR3H46ZnohCSC/5psyS47avRzpah2Y8zFOYmYB5u0Rf6Jwa02jI0cMrohEJteXnSkik+yzv5s1Cx6slRmzpRpTW0qrvbGFY6+tD6GtP1en/XnJouu35vGxhWuIjPPbqSipk2BXGFwRiczG8wAavvjLshkCqXBocrI3k7aFW2U7AgRJAp3ym1SpBNw3Y0LsJDOagJvafCo0Ot3kbVJll+JNn/PLUrHh73YWoBQrVYhOY5NzUzC4kjmOlGd7WKhH5dnS5LBku1KyDevfYKtXY/ka4M/+0D2KoCE+tlIgKgY+PmzHnH+uSp0EUW3SMcKuGJdkRFyGCGuRL8mDq5UrV8LPzw/u7u7o3bs3QkNDdS575coVDB8+HH5+flAoFFi2bJnZ65S76LRck38rv9GDiKhsJDQiMo25owjeuWd6oWVRie3Ueksdl2UXWH6uKG2qCkiZMzJMZLJ5hfvhegKooStPGbSOraGxZqVBKpIGV9u2bcPkyZMxa9YsnD9/Hp07d0ZAQABSUlK0Lp+Xl4cWLVpgwYIF8Pb2FmWd9owDWkiDJY2kT4yFmlnwsqtMEMxrAlbeTpGHBZcMH1BmWX7EuoUjtt7XjByXsRN3azN15yUUK22nQKOMpMHV0qVLMW7cOIwdOxbt27fH6tWr4eHhgd9++03r8j179sTixYsxcuRIuLm5ibJOIrKO64nWmY1ebo5GplaavJQsR1UheMgqML1vTnmTt0eY1ZJAjthkVf62no0z6/eOeI6rKlh2vCNiGl3XjrXj/YrPdFsgWXBVVFSEc+fOwd/f/2FinJzg7++P4OBgq66zsLAQWVlZGv+ISFwbdLTfdgQRd03vfE3G+Ts8wWLrTjWwX5Mpgm6kWmzd1pCeW4QDV5JwLsYyAzfIqQJHjvP36Do++ppm2bNriVlYsO86MvPFKVxxRLoCe9sLdazPRaoN37t3D0qlEl5eXhqfe3l54fr161Zd5/z58zFnzhyTtklEJAeOWEKtTeB122wCbq3TZ6nNdJt3yCLr/WDLBaTnFsqq9Lr7V4elToLBCm2oj5hYBAEY/P0JAKWB8OJXO0ucIvtTrFShmrPkwzbIFo8MgGnTpiEzM1P9Ly7OvGp4uZDRu4iIyObZwzPV3H2w9jH4JyIBp26lQcz5SJOzCnHcxmsKDeXsJKc6P+sof6lcTihtiVSsVCGnsHSADcc7IuK6lpiF1l/uE6VPlSFs8bkrWc1VgwYN4OzsjORkzZOTnJysc7AKS63Tzc1NZx8uImPFpnNeCHJc+UVKBEXaZu2RIxCjhtMeBlkY/ZvtjiJsDEvEVqdvp4m/Ugt7ekkQ7t7PR8TMgVInxW6MWx8mdRJkS7KaK1dXV3Tv3h2BgYHqz1QqFQIDA9GnTx/ZrNOW2cMLkIhsy7awOIz5/azUySALssWSZEeUV6REiZInCwDu3i8dVOhsdDr7DJHFSVZzBQCTJ0/Gm2++iR49eqBXr15YtmwZcnNzMXbsWADA6NGj0aRJE8yfPx9A6YAVV69eVf9/fHw8wsPDUbNmTbRq1cqgdToSvgCJyJGJ/Qg0dKJeexZxN9NqzYHIfP/9JUTqJFhd+bwP+6KSFCQNrkaMGIHU1FTMnDkTSUlJ6NKlC/bv368ekCI2NhZOTg8r1xISEtC1a1f130uWLMGSJUvQv39/BAUFGbROIiJ7xDwEGcLcy2R54E1R0kHi0tVSRSlmZzUZC4u2zCiVpJtCYZ33ji22wpI0uAKAiRMnYuLEiVq/KwuYyvj5+RlUCqFvnURERERkPxI1Jgt3jICyvIJipdRJsBhbLDjkaIF2bFNIrNRJICIiGdFoMiVdMohIRG1n7Jc6CVQOgyuZU3DQUCIiEgkDKvuksMW2Uxai2eeq8vdxHNGXLIzBFRERkYNZfex2haZUJFfjN56rchkO3PCQoPH/lY/LrvAE6yWGHBKDKyIiIgchCAIKipVYsO+61EkhA+27nCR1EmzWjeQcjcDz7n3WWomF8bxuDK5kTlupCxFRRXxWVMbSfO1UPC52x9GbBerb/d3laqpm/3PVCqkhR8fgiojIDjC/TIbitWI/NofG4uj1FKmTISsVC1X+uhAvUUrIUTG4kjkOaEFERETabA6Jxdi1Z5lTKOd+XrHUSSARbQm1vZGvGVwREdkBB28VpNO5mPtSJ0FWWGtFRLZkjg025WRwRURkB5hp1m7azktSJ0FW2DePiMiyGFzJHEujiYiISC/mFYhkg8GVzLE0mojINMsO35Q6CbIjCJxI2C45+Ell/3SSEwZXRER2gAUxla04ekvqJBARkRaFJUqpk2AxDK6IiOwAYysiB8aKG7Ix287GSZ0Ei2FwJXPsc0VERGJhEE5EcpCVb79D5jO4IiIichBsPmqf8grtt4kVka1hcEVERORABEZYdmfDmRipkyCpVcd0969kCyB5sufHEIMrIiIiB8F5rsgeXY7P0vldUGSqFVNCxOCKiMgusDaCiIhsxT8XE6ROgsUwuCIiInIQjMGJSA5uJOdInQSLYXBFRETkQNJzi6ROAhGR3WJwJXPsh0lEhmCFBBlq/MbzUieBiGTsXMx9qZNg0xhcyRwzTEREJKaribo7/xMRDV91Wuok2DQGV0REdqBYqZI6CURERA6PwRURkR1YsO+61EkgG5BVUCx1EoiI7BqDKyIiO5BdUCJ1EsgGRN/LlToJRER2jcGVzHFACyIiIiIi28DgioiIyEEoWGRHRGRRDK6IiIgcxImbqVIngYjIrjG4IiIichC5RUqpk0BEZNcYXBEREREREYmAwRUREREREZEIGFwRERERERGJgMEVERERERGRCBhcyZxCwWFziYiIiIhsAYMrmRMEQeokEBERERGRARhcyVx0Wp7USSAiIiIiIgMwuCIiIiIiIhIBgysiIiIiIiIRMLgiIiIiIiISAYMrIiIiIiIiETC4IiIiIiIiEgGDKyIiIiIiIhEwuCIiIiIiIhIBgysiIiIiIiIRMLgiIiIiIiISAYMrIiIiIiIiETC4IiIiIiIiEoEsgquVK1fCz88P7u7u6N27N0JDQ/Uuv2PHDrRt2xbu7u7o2LEj9u7dq/H9mDFjoFAoNP4NGjTIkrtAREREREQOTvLgatu2bZg8eTJmzZqF8+fPo3PnzggICEBKSorW5U+fPo1Ro0bh7bffxoULFzBs2DAMGzYMly9f1lhu0KBBSExMVP/bsmWLNXaHiIiIiIgclEIQBEHKBPTu3Rs9e/bEihUrAAAqlQq+vr744IMPMHXq1ErLjxgxArm5udizZ4/6s8cffxxdunTB6tWrAZTWXGVkZGDXrl0GpaGwsBCFhYXqv7OysuDr64vMzEzUrl3bjL0zn9/UfyXdPhERERGRVKIXDJE6CcjKyoKnp6dBsYGkNVdFRUU4d+4c/P391Z85OTnB398fwcHBWn8THByssTwABAQEVFo+KCgIjRo1Qps2bTB+/HikpaXpTMf8+fPh6emp/ufr62vGXhERERERkSOSNLi6d+8elEolvLy8ND738vJCUlKS1t8kJSVVufygQYOwfv16BAYGYuHChTh27BgGDx4MpVKpdZ3Tpk1DZmam+l9cXJyZe0ZERERERI7GReoEWMLIkSPV/9+xY0d06tQJLVu2RFBQEJ555plKy7u5ucHNzc2aSSQiIiIiIjsjac1VgwYN4OzsjOTkZI3Pk5OT4e3trfU33t7eRi0PAC1atECDBg1w69Yt8xNNRERERESkhaTBlaurK7p3747AwED1ZyqVCoGBgejTp4/W3/Tp00djeQA4dOiQzuUB4O7du0hLS4OPj484CSciIiIiIqpA8qHYJ0+ejDVr1mDdunW4du0axo8fj9zcXIwdOxYAMHr0aEybNk29/EcffYT9+/fj22+/xfXr1zF79myEhYVh4sSJAICcnBx89tlnOHPmDKKjoxEYGIihQ4eiVatWCAgIkGQfiYiIiIjI/kne52rEiBFITU3FzJkzkZSUhC5dumD//v3qQStiY2Ph5PQwBnziiSewefNmTJ8+HV988QVat26NXbt2oUOHDgAAZ2dnXLx4EevWrUNGRgYaN26MgQMHYt68eexXRUREREREFiP5PFdyZMxY9pbGea6IiIiIyFFxnisiIiIiIiIHxOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiMpxUkidAiIiIiKyVQyuiIiM1LSeh9RJICIiIhlicCVzCtakEMnOs+29pE4CEZHFtfepLXUSiGwOgyuyOQGPac/YNqlT3ex1K+wwml00vJPUSVBzdbGPR85zHX2qXGbN6B5WSAkBQC03F6mTQKTVm32aSZ0Es/h4ukudBCKbYx85HTtW05WZhoq+fqmj1s8fa2x+CZspodXXL3Uwe7uW9FpPX5yf8azUyQAA1HB1ljoJoqiqb15jT3c82bqBdRJjIz4LaGOR9T7qVRM//a+7RdZtrwa0aSh1EhzGrBcekzoJas+0bSR1EogcAoMrudOSifvk2UeNWkU7G6rW/3N8nyqXqaEj4Jz/cke81be5WU0pWzWqafRv+rSob/oGLezxFvUAAC7ODw/KF8+1lSQtRz7pDxdn+3jk1PVw1fu9HGtAuzerq/7/vyY8YfJ6Zr3Q3ujf9Gpez2JNnF/t7gvBMqu2W7oKqKTWoYntvKsMJcNHgcV18a0jyXYPTHpKku1qM6KHr9RJsFk1XJ2x5NXOUifDLPaR07Fn5XINUwe3xfV5g/DBM62xfFRXg1ex98N+eK9/S4OW1VYL8+F/Whm8LXN1b1avymWq66j9qF/TDTNfaI/Hm5se7KwZ3cPoGgcnGb89X3vwgK9e7eExe713M40M8rq3epm1jWrOhu1/4zrVZTMQxOb/642fzajt8GtQA18N64AxT/hp/b5ZfQ8oVZpZfqmbrpU/Sz6epjehHdu3uVHLz3qhPTa83QuChSIgJwsM8XlhxrN456kWmDdM3rXSpjK3CbU5wbk+/2ljfzUrxha0tG5UExdmPGuRfp3/aaf/+NbxqIa6HtXM3s6qN7phy7jHzV6PsRrXsVwTRn33zKvdH6n02bTn2lb6TS13tkQyxIA2jTSao/49sa+EqTENgysb8l7/lnB/kEl+sXNjg36z6/2+UCgUmDrYsNqK13trtg9/uVsTTB6ouznP+AEtETFzoPrvd59qoXPZKYMqr8fY9uhlD6vxA3QHi4teMb2PkW89D2x4uzeiFwzBtbmDUKfci8artpvW3+h7d/ZpUR/RC4Zgkn9rnctoezCbo/wxHdalCQCgmrMT9nzQD39P7Isabi4aGV13Fyf41Tc96NGWae7pV7fyhwCWjehi8nZMMXVwW7zUtUmlz59o1QADH/M2a91vPN4Ms198DNELhuDQx5olpt++1hk13FwwpFzfrP8Zca0bErCufkN/cPj5IM173kmhwJ4P+mHLuMfhbUQ/igY1tV/3hhrdxw9uLpULRFo0qIHOD0q4X+th2D3QomENrZ/pKnAxlZOTAl881w7/e1z3OfOsbnwmVNczRBsxMrnlPddR9/XuryXTra+QqbYJ+w5ofy486lUT/2nbCFMGtcG7BhQCbnjbsMKg38YY3+dR7GbLQ7uUvqeDp/0Hv4/tiUa19J//70Z0xqHJ/VG3hmuV97cpevlVXXjp7KSZLexvYBPSxg+eKSv/2w0+ntXRp6XuQk5XC7VgqOVeDS9red7r4l7NCYcnV13btfv9vgjQ877Qlueo4+GKjf/XGyN7+mLJq52x4OWO6GnA8TfGj693M2r5aVXkAxvU1N8iQxdzmxm7VeiLLUBAG+9a6r87PVLHrPVLQRbB1cqVK+Hn5wd3d3f07t0boaGhepffsWMH2rZtC3d3d3Ts2BF79+7V+F4QBMycORM+Pj6oXr06/P39cfPmTUvugsV0aOIJQHtm6z962k9/PqgtohcMMb96vorS5v6PNoSnRzWEz3wWp6f+B9Oea4eTnz+N1W90w473+qBlwxo4PLk/ohcMwYQBmjVgl+cEYM7QDtgy7nE0reehfmmWH7BCV1CiLTNQxreeB05MedrgGhWFojTAOfulv8bn1V2d8Wy7h2k5MeU/OP7Z05V/DwUaPnhpulfTvKXKAq9J/o9i9RvdtdYCvtStCW58NVjjs5e7NdFbLT7nxcfg4+mOzr514OPprtH0c9YLpRn+6AVDNEr1OzTxVD+kujato7G+re/ob47ZtJ4Hapcrdauq+eTG/+uN1W90xx/vaa73kbrGlZi/81QLnJjytFF9ar4qV9vwavdH8F2FgM4SNbGtvWppZFzKaoZWvt4NoV8+gxX/7YqPn30UE59upTdTfnrqfxD51SDs++jJSt+Vv7b+93gzDOrgjelD2ulc11v9/DT+HtPXDx2aeFbK9FSsfStfoLDurV4Im+6Pq3MD8OnAR3HsswE6t6eL84NrsGW5wGjPB/2w96MnsW5sT3w/sgvmvKi7hqj8ICh7PuhX6fsBjzZEVx3Puar6mJSv0dVlSIXBS87PeBYf/qcV/p7YF2/301+L91bf5rg4eyBmvdAeo3r5YlsV91l5M55vr36umGLZiC74c/wTGD+gJc5N98ePr3fHD6O6VjqGjT3dsarCvfr7mJ5Y/1YvHJj0FL4f2UVrUGuskT19MXdo5fN88OP++G1MT0wY0Ao13Fy01gbv+aAfPKtXw4QBLfFk68oZud/G9NCojZ/z4mP4T1vNmp/N/9db4++o+c/hzjfPaQSdL3VrorcgzBhDOvng+5GlLUx8PKvj6TaNKr1OL80eqPG3olz9srOOGtnmDWrg0MdPYfO43lq/18eQSrTyy6x+oxte790Mf45/WFOp6/l15NMBCPniGQzppH2wn/893gxPt2mIqPnP4ercABz8WHdQU/FdXF7ErIF4RUuBZFkwulRLAV7/R7Vn/tt610arRg8z8RXv9TKdfevgAxPeG80b1MCC4Z3wSvdHMLJXUwhmVt+/3E0zcHyydQO4ujhpPScV93n6kHZaCy82vN0L29/tgymD2uCXN3tq3e5zHb2x+g3dgdyK/3ZD2HTd5wwofZ7pEvqFv0aTzlYNa6JBTTecmPI0Lsikv7ixJA+utm3bhsmTJ2PWrFk4f/48OnfujICAAKSkpGhd/vTp0xg1ahTefvttXLhwAcOGDcOwYcNw+fJl9TKLFi3C8uXLsXr1aoSEhKBGjRoICAhAQUGBtXZLNN+P7II3+zTD3g8rZ7bWjO6Bas4KeNd2x7HPBuCXcqOTaSshLctQj+7TDM5OCrT3qY1+rR6WTr7euykAaDxIy0r3j302AP7tGmm8oKpXc1aXxNTxcEXjB7VKj9T1wKAOPujpVw+BnwzQyIiXPQTmv9wRNR80k+rTsj6OT3la/dJsUudhLcok/0fRutzvyzrFd29WD3+O74M5Lz6GUb2aYlOFF6dvPQ+ETX9WI/P0XEdvrf2NXun2CBa/2llrRqZ8kz9XFyc0re+hTneZhrXcsPn/emNgey/sHN9XfRwBzRfVoA7eWmsB/erX0MhA9vSri6WvdcEr3R9Rj/Q3b1hpENrpEU/8M7Ef3nzCD8HTnsHu9/vi9NT/YFSvh+27DWkm1bXpwxJkhUIBb093vSWlx6c8jYhZA1HLzQVOCs0RpD6u0AcwbLo/3FycMaiDN3r41cOIHr4Y84Qf3Ks5Q6FQqGtUOj/iid3va6/u96vvgdkvtMcXz7WDbz0PDGzvhblDDesYPqSjD2q5uaBFwxqo/6DWpVfzh4FP+XNgaI2uIS3PympmKwaQjWq54/lOjVHN2QmfBrRBxKyBeKrci2/V690wyb81ImYOROM61eHm4qzxwgeATwc+imtzB+HpNg0xooevurna/z3ZAtELhmjU/FycPRARMwdq1BYpFJVHOCyrNXq3fwv1fTJ1cFsEfTpAvUyHB4PEeLi6YOJ/WqNZfc1Mdvlawb6tHgZt2kYYC3jMGzOeb48/3uuDDk084V7NGXU8XDG0SxNUd3XW2mzyydYNEDz1P9jwdi+cm+4PD1cXjYzV54PaQqFQQKFQ4MCkpyplAOa/3BEvdG6M7e/2QfSCIYia/5zGs0JXTZJLuRO+dMTDQo5uTeugXg1XTB7YBs3q18CHz7TW+psyM19oj9ru1TC2b3PMf7kT/BrUwOAOpc/UUb181TUbALDx7d54t38LfDWsA97s0wxDuzTBd691AVA5AC4ztq/2z5eN6IJhXZuge7O6+HxQW/V98ELnxuoCuzICSmu2e5QvVVeUPhfaeNfC0C5N4K6l5rFiX9OqWiFMf7492vnUxokplQuoyitRqTT+fra9Fzo08UTErIGY8uDZsaNCoc3TbRphbN/miJr/HM7PeBZvajleT7RqoH5/9WvVAAqFAk5OCswuN+BEkzoemOT/qEZT6S6+ddColhs+HVi5r3P0giGV3hvfj+yCzwLa4Bst/domPq2ZQa/lXg07yzWx9KhQc/aoV+m7r3xz6sWvdEJrr1p4oqVmzaK2Aqj/Kxf8t25UE371a6BJnepoW65W4GP/R7Hk1c6o5eaCn97oDt9yz69BHXzg7KRA92Z18dP/uuORutXx+9ieOP7Z0xjVq6nGttxcnOBVW3uN+Ou9m2LesA74fWwvKBQKuDg7aTwjKtbANKzlhtAvnqlUiPrtq53hWb1apUKTyc8+ikEdHgbJEbMGwrde6X4MaNMQ697qhegFQyql678P9mH9W70Q8JgXZr34MAB44kEhVNnzvG4N10oZ/dVvdMOFGc9CoVBg3Vu9MPHpVuj0iKfOQKR+uVYAhgS6jWq5oZ1PbfX5L/+8bVKnOmq5V8PFWQNxbro//nivD3a/3xefBbRBLXcXzHi+nboZ4qb/643/e7K0VVH5vMKM59ujX6sG6NW8HiYMaIUuvnWw9LXO+HxQW4337YQBrTCogw+iFwzB4clPaeTJgNJnX4OaboiYNRCX5wSoazHLb+ftfs1xYsrTWmvQPT2qoY13Lfw5/gm8178lJjy4T3zreaBuDdNq06SmEMwNpc3Uu3dv9OzZEytWrAAAqFQq+Pr64oMPPsDUqVMrLT9ixAjk5uZiz5496s8ef/xxdOnSBatXr4YgCGjcuDE++eQTfPrppwCAzMxMeHl5Ye3atRg5cmSVacrKyoKnpycyMzNRu7a8O9gWK1VQAOqBAvZfTsSZO+mY8Xz7SiVfeUUluJaYha6+dVGiEtQZekEQkFVQgtruLur24Zn5xbiVkoNuTetUajPuN/VfAKX9Eoy98FOzC3HxbgaebtNIZxAQGpWO134KRsNabjj7pT/upOZg1t9XMLJnU52lYroUlajg7KRAUYlK3XToyPVkfLwtAqP7NENWfjEmP9sGnjqa4Gw8E4Ppu0oD97KHc3ZBMe7lFKGWuwtUKgGNtLxQHpu5H7lFSsx+oT3GVOij8s76MBy8mqz+u2y9Q1ecRMTdTCx6pZO6r5ShipUqLDt8A/1aNdTbHKO8svN4eup/1IFx2WflNavvgWMPauwKS5QQBGDFkVtYcfQWgNIS4OWBt/Dd4RsY3u0RfPta1R1REzLy0bCWG6o5O6GopDTtPwbdBgBse+dx9NYxSMj52Ptwc3HCvxcT0a91A/x3TYjG967OTrjx9eBKvys7jw1quiJsuuYL8teTURAEAb71PDD3n6tYPqoLgm+nQRCAx5rUxuaQWMx/uRNcnBQY+fMZDOvaRGez1DupOfDxrF5lM7XbqTl45ttj6NGsLv4Yr73/yl8X7mJDcAxWvdFdZ6alzC8n7uCrf68BgEYmoux8+tX3QFCFWlelSkBeUQlquVfD/dwixKbnqQOunMIS5BWWaL22AWDFkZu4k5qLb1/rjHHrw3D4WgrOTfdH/ZpuKFaqMG59GIIiUyulR5/4jHz0XXAEQGnt9ZwXO2htvliiVOF6UjY8q1fDI3Wr63w+6dv2wStJOHXrHnzreaiP229jeuCttWEY0tEHKytk9mLT8rAxJAZv92te6VxM/fMi7uUU4ttXu2BXeDwGd/TGLyei0K1pHQzqUPl5VViixKW7mejiWwcuzk44F3Mf2QXFGKCjv1FBsRLu1ZzxyfYI/Hn+rvrz/o82xOJXOqHXN4EAgF/f7IHbqTmo4eaCUT2bVlnI0nH2AWQXlGBYl8ZY9qCGpezYnZr6H43+IltDYzF15yX139fnDYKrsxOyCorxxq8h8KtfAyv+2w2RSdkIWHYcb/drjl9PRmls79bXg9XvqY1nYrDs8A38NaEvfCv0w/wnIgEfbLkAAJg3tLTwTNtAOBdi72NraBymPdcWdXQMMLPm+B18vfca1r3VS12Sn5pdiHo1XDXej0euJ+PQ1RTMeqG9uum9NhWfj9ELhiAtpxDz9lzFrvAE9We6CIKA26m5AATUdKumvr7f3RCGy/FZCPykv8b203OLcOJmKgIe80b6g3v08XLPxicXHUFcej6eaFkfm8c9DqVKwNHrKfi/9WEaaUnJKkAdD1e4ujihRKmCk0KBvyMScCc1Bx8/+ygUCgVUKgFOTgrEpedhzj9XMe7J5jqfw2X78uf5eHy6IwIAcOeb5ypdc2XH6/uRXTC0S+Xmej8duw2VUNrMP7ugGENXnsKz7bww7bnSGvnkrAK8ve4sXJyc0LJhTSx6pROcnRQQBAHBd9IQl56H26m5+HxQW601fZn5xaUFgg++2385CTN2X8aUgDZo610bHZrUrvT8UKoEpGQX6OyXmlVQDFdnJ7i5OBndn+5eTiGeXhwEd1dnHPr4KewOT8DjLerDvZoTgiJT0bVpHby44pR6+aj5z0GhUKCgWIn4jHy0bFgT28PisORAJH4b07NSQUmZsnOZkVeE6LQ8jRZMhSVKfH/4JoZ08sFjjbX/vkyxUoW0nKJKz+HMvGLsjohHanYhvGq7440KzadTsgoQFJmKFzo3hns1zeOUmJmP/1sXho5NPLH1bBye6+iNH1+3jdFejYoNBAkVFhYKzs7Owl9//aXx+ejRo4UXX3xR6298fX2F7777TuOzmTNnCp06dRIEQRBu374tABAuXLigscxTTz0lfPjhh1rXWVBQIGRmZqr/xcXFCQCEzMxMk/bL3qVkFQixabkW3cb1xCwhu6DYYutXqVQGLVeiVAm/n7wjXE0w7lpIzS4QDl5JEkqUlbejUqmEEqVKSMjIE/IKS9Sf5xWWCOGx9w1Om7muJmQKwbfvaXwWGpUm/O/XEGFbaKwQm5YrfLP3qtZznV9UInx7MFK4dDdD/VlyZr5ZaT9yLVn4+dhto9ax7nSUMG7dWWH235eF7vMOCkmZ+VqXK1GqhMBrSUJqdoHJ6RObmOe5sFgpLDt0Qzgfk67x+afbw4Vmn+8Rdl24K9q2DBGblisEfHdM2BEWZ9Tvpu28KLz1e6hZx+bEjVSh2ed7hO8ORVa5bF5hiTBs5Ulh+eEbJm/PWlQqlXDm9j0hNbtACItOEwqKS4SC4hKh2ed7hGaf7xGKSpRGrS/mXq6w4shNITO/SP3Z3ft5wpX4ys86lUolXInPFO7nFgoZeUWVvtNm14W7wvazpc+RuHTD3xcqlUrYdynBqN/oU1BcUvVCBlpx5KbQ7PM9whu/nNF49imVKmFraIxwIynLpPWqVCpBqeVdUZW79/OE7w5FVnquLT0YKWwJiTEpLcZQqVTCJ9vDhXn/XNH6fVBkirDkwHWT9s1R7QiLE+b+c0XvM9BaeQRLyy8qsal9yczMNDg2kLTmKiEhAU2aNMHp06fRp8/Dav4pU6bg2LFjCAkJqfQbV1dXrFu3DqNGjVJ/9uOPP2LOnDlITk7G6dOn0bdvXyQkJMDH52Gp4WuvvQaFQoFt27ZVWufs2bMxZ86cSp/bQs0VEZUqK62jhwRBQHJWoVEDWJBtSc0uhEJh/sAjZJj7uUU221SJiExnTM2V5H2u5GDatGnIzMxU/4uLi5M6SURkJAZWlZX1pyP71bCWGwMrK2JgRURVkTS4atCgAZydnZGcnKzxeXJyMry9tQ976e3trXf5sv8as043NzfUrl1b4x8REREREZExJA2uXF1d0b17dwQGBqo/U6lUCAwM1GgmWF6fPn00lgeAQ4cOqZdv3rw5vL29NZbJyspCSEiIznUSERERERGZS/LpoidPnow333wTPXr0QK9evbBs2TLk5uZi7NixAIDRo0ejSZMmmD9/PgDgo48+Qv/+/fHtt99iyJAh2Lp1K8LCwvDzzz8DKG0GM2nSJHz11Vdo3bo1mjdvjhkzZqBx48YYNmyYVLtJRERERER2TvLgasSIEUhNTcXMmTORlJSELl26YP/+/fDyKp0AMDY2Fk7lZgx/4oknsHnzZkyfPh1ffPEFWrdujV27dqFDh4eTE06ZMgW5ubl45513kJGRgX79+mH//v1wd2ffAyIiIiIisgzJ57mSI1ua54qIiIiIiCyHowUSERERERFZGYMrIiIiIiIiETC4IiIiIiIiEgGDKyIiIiIiIhEwuCIiIiIiIhIBgysiIiIiIiIRMLgiIiIiIiISAYMrIiIiIiIiETC4IiIiIiIiEoGL1AmQI0EQAJTOxkxERERERI6rLCYoixH0YXClRXZ2NgDA19dX4pQQEREREZEcZGdnw9PTU+8yCsGQEMzBqFQqJCQkoFatWlAoFJKmJSsrC76+voiLi0Pt2rUlTQtZF8+94+K5d1w8946L595x8dzLnyAIyM7ORuPGjeHkpL9XFWuutHBycsIjjzwidTI01K5dmzecg+K5d1w8946L595x8dw7Lp57eauqxqoMB7QgIiIiIiISAYMrIiIiIiIiETC4kjk3NzfMmjULbm5uUieFrIzn3nHx3DsunnvHxXPvuHju7QsHtCAiIiIiIhIBa66IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMrmRu5cqV8PPzg7u7O3r37o3Q0FCpk0RGmD9/Pnr27IlatWqhUaNGGDZsGCIjIzWWKSgowPvvv4/69eujZs2aGD58OJKTkzWWiY2NxZAhQ+Dh4YFGjRrhs88+Q0lJicYyQUFB6NatG9zc3NCqVSusXbvW0rtHBlqwYAEUCgUmTZqk/ozn3b7Fx8fjjTfeQP369VG9enV07NgRYWFh6u8FQcDMmTPh4+OD6tWrw9/fHzdv3tRYR3p6Ol5//XXUrl0bderUwdtvv42cnByNZS5evIgnn3wS7u7u8PX1xaJFi6yyf6SdUqnEjBkz0Lx5c1SvXh0tW7bEvHnzUH7sMJ57+3D8+HG88MILaNy4MRQKBXbt2qXxvTXP844dO9C2bVu4u7ujY8eO2Lt3r+j7S0YQSLa2bt0quLq6Cr/99ptw5coVYdy4cUKdOnWE5ORkqZNGBgoICBB+//134fLly0J4eLjw3HPPCU2bNhVycnLUy7z33nuCr6+vEBgYKISFhQmPP/648MQTT6i/LykpETp06CD4+/sLFy5cEPbu3Ss0aNBAmDZtmnqZO3fuCB4eHsLkyZOFq1evCj/88IPg7Ows7N+/36r7S5WFhoYKfn5+QqdOnYSPPvpI/TnPu/1KT08XmjVrJowZM0YICQkR7ty5Ixw4cEC4deuWepkFCxYInp6ewq5du4SIiAjhxRdfFJo3by7k5+erlxk0aJDQuXNn4cyZM8KJEyeEVq1aCaNGjVJ/n5mZKXh5eQmvv/66cPnyZWHLli1C9erVhZ9++smq+0sPff3110L9+vWFPXv2CFFRUcKOHTuEmjVrCt9//716GZ57+7B3717hyy+/FHbu3CkAEP766y+N7611nk+dOiU4OzsLixYtEq5evSpMnz5dqFatmnDp0iWLHwPSjsGVjPXq1Ut4//331X8rlUqhcePGwvz58yVMFZkjJSVFACAcO3ZMEARByMjIEKpVqybs2LFDvcy1a9cEAEJwcLAgCKUPcCcnJyEpKUm9zKpVq4TatWsLhYWFgiAIwpQpU4THHntMY1sjRowQAgICLL1LpEd2drbQunVr4dChQ0L//v3VwRXPu337/PPPhX79+un8XqVSCd7e3sLixYvVn2VkZAhubm7Cli1bBEEQhKtXrwoAhLNnz6qX2bdvn6BQKIT4+HhBEAThxx9/FOrWrau+Hsq23aZNG7F3iQw0ZMgQ4a233tL47OWXXxZef/11QRB47u1VxeDKmuf5tddeE4YMGaKRnt69ewvvvvuuqPtIhmOzQJkqKirCuXPn4O/vr/7MyckJ/v7+CA4OljBlZI7MzEwAQL169QAA586dQ3FxscZ5btu2LZo2bao+z8HBwejYsSO8vLzUywQEBCArKwtXrlxRL1N+HWXL8FqR1vvvv48hQ4ZUOjc87/bt77//Ro8ePfDqq6+iUaNG6Nq1K9asWaP+PioqCklJSRrnztPTE71799Y4/3Xq1EGPHj3Uy/j7+8PJyQkhISHqZZ566im4urqqlwkICEBkZCTu379v6d0kLZ544gkEBgbixo0bAICIiAicPHkSgwcPBsBz7yiseZ75HpAfBlcyde/ePSiVSo2MFQB4eXkhKSlJolSROVQqFSZNmoS+ffuiQ4cOAICkpCS4urqiTp06GsuWP89JSUlar4Oy7/Qtk5WVhfz8fEvsDlVh69atOH/+PObPn1/pO553+3bnzh2sWrUKrVu3xoEDBzB+/Hh8+OGHWLduHYCH50/f8z0pKQmNGjXS+N7FxQX16tUz6hoh65o6dSpGjhyJtm3bolq1aujatSsmTZqE119/HQDPvaOw5nnWtQyvA+m4SJ0AIkfx/vvv4/Llyzh58qTUSSELi4uLw0cffYRDhw7B3d1d6uSQlalUKvTo0QPffPMNAKBr1664fPkyVq9ejTfffFPi1JElbd++HZs2bcLmzZvx2GOPITw8HJMmTULjxo157okcBGuuZKpBgwZwdnauNHpYcnIyvL29JUoVmWrixInYs2cPjh49ikceeUT9ube3N4qKipCRkaGxfPnz7O3trfU6KPtO3zK1a9dG9erVxd4dqsK5c+eQkpKCbt26wcXFBS4uLjh27BiWL18OFxcXeHl58bzbMR8fH7Rv317js3bt2iE2NhbAw/On7/nu7e2NlJQUje9LSkqQnp5u1DVC1vXZZ5+pa686duyI//3vf/j444/VNdg8947BmudZ1zK8DqTD4EqmXF1d0b17dwQGBqo/U6lUCAwMRJ8+fSRMGRlDEARMnDgRf/31F44cOYLmzZtrfN+9e3dUq1ZN4zxHRkYiNjZWfZ779OmDS5cuaTyEDx06hNq1a6szcH369NFYR9kyvFak8cwzz+DSpUsIDw9X/+vRowdef/119f/zvNuvvn37Vppy4caNG2jWrBkAoHnz5vD29tY4d1lZWQgJCdE4/xkZGTh37px6mSNHjkClUqF3797qZY4fP47i4mL1MocOHUKbNm1Qt25di+0f6ZaXlwcnJ82slbOzM1QqFQCee0dhzfPM94AMST2iBum2detWwc3NTVi7dq1w9epV4Z133hHq1KmjMXoYydv48eMFT09PISgoSEhMTFT/y8vLUy/z3nvvCU2bNhWOHDkihIWFCX369BH69Omj/r5sSO6BAwcK4eHhwv79+4WGDRtqHZL7s88+E65duyasXLmSQ3LLTPnRAgWB592ehYaGCi4uLsLXX38t3Lx5U9i0aZPg4eEhbNy4Ub3MggULhDp16gi7d+8WLl68KAwdOlTrMM1du3YVQkJChJMnTwqtW7fWGKY5IyND8PLyEv73v/8Jly9fFrZu3Sp4eHhwOG4Jvfnmm0KTJk3UQ7Hv3LlTaNCggTBlyhT1Mjz39iE7O1u4cOGCcOHCBQGAsHTpUuHChQtCTEyMIAjWO8+nTp0SXFxchCVLlgjXrl0TZs2axaHYJcbgSuZ++OEHoWnTpoKrq6vQq1cv4cyZM1IniYwAQOu/33//Xb1Mfn6+MGHCBKFu3bqCh4eH8NJLLwmJiYka64mOjhYGDx4sVK9eXWjQoIHwySefCMXFxRrLHD16VOjSpYvg6uoqtGjRQmMbJL2KwRXPu337559/hA4dOghubm5C27ZthZ9//lnje5VKJcyYMUPw8vIS3NzchGeeeUaIjIzUWCYtLU0YNWqUULNmTaF27drC2LFjhezsbI1lIiIihH79+glubm5CkyZNhAULFlh830i3rKws4aOPPhKaNm0quLu7Cy1atBC+/PJLjaG0ee7tw9GjR7W+3998801BEKx7nrdv3y48+uijgqurq/DYY48J//77r8X2m6qmEIRy04YTERERERGRSdjnioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiIiIiIhEwOCKiIiIiIhIBAyuiIiIiIiIRMDgioiIiIiISAQMroiIiIiIiETA4IqIiIiIiEgEDK6IiMgupaamYvz48WjatCnc3Nzg7e2NgIAAnDp1CgCgUCiwa9cuaRNJRER2xUXqBBAREVnC8OHDUVRUhHXr1qFFixZITk5GYGAg0tLSpE4aERHZKdZcERGR3cnIyMCJEyewcOFCPP3002jWrBl69eqFadOm4cUXX4Sfnx8A4KWXXoJCoVD/DQC7d+9Gt27d4O7ujhYtWmDOnDkoKSlRf69QKLBq1SoMHjwY1atXR4sWLfDHH3+ovy8qKsLEiRPh4+MDd3d3NGvWDPPnz7fWrhMRkYQYXBERkd2pWbMmatasiV27dqGwsLDS92fPngUA/P7770hMTFT/feLECYwePRofffQRrl69ip9++glr167F119/rfH7GTNmYPjw4YiIiMDrr7+OkSNH4tq1awCA5cuX4++//8b27dsRGRmJTZs2aQRvRERkvxSCIAhSJ4KIiEhsf/75J8aNG4f8/Hx069YN/fv3x8iRI9GpUycApTVQf/31F4YNG6b+jb+/P5555hlMmzZN/dnGjRsxZcoUJCQkqH/33nvvYdWqVeplHn/8cXTr1g0//vgjPvzwQ1y5cgWHDx+GQqGwzs4SEZEssOaKiIjs0vDhw5GQkIC///4bgwYNQlBQELp164a1a9fq/E1ERATmzp2rrvmqWbMmxo0bh8TEROTl5amX69Onj8bv+vTpo665GjNmDMLDw9GmTRt8+OGHOHjwoEX2j4iI5IfBFRER2S13d3c8++yzmDFjBk6fPo0xY8Zg1qxZOpfPycnBnDlzEB4erv536dIl3Lx5E+7u7gZts1u3boiKisK8efOQn5+P1157Da+88opYu0RERDLG4IqIiBxG+/btkZubCwCoVq0alEqlxvfdunVDZGQkWrVqVemfk9PDV+aZM2c0fnfmzBm0a9dO/Xft2rUxYsQIrFmzBtu2bcOff/6J9PR0C+4ZERHJAYdiJyIiu5OWloZXX30Vb731Fjp16oRatWohLCwMixYtwtChQwEAfn5+CAwMRN++feHm5oa6deti5syZeP7559G0aVO88sorcHJyQkREBC5fvoyvvvpKvf4dO3agR48e6NevHzZt2oTQ0FD8+uuvAIClS5fCx8cHXbt2hZOTE3bs2AFvb2/UqVNHikNBRERWxOCKiIjsTs2aNdG7d2989913uH37NoqLi+Hr64tx48bhiy++AAB8++23mDx5MtasWYMmTZogOjoaAQEB2LNnD+bOnYuFCxeiWrVqaNu2Lf7v//5PY/1z5szB1q1bMWHCBPj4+GDLli1o3749AKBWrVpYtGgRbt68CWdnZ/Ts2RN79+7VqPkiIiL7xNECiYiIjKBtlEEiIiKAfa6IiIiIiIhEweCKiIiIiIhIBOxzRUREZAS2piciIl1Yc0VERERERCQCBldEREREREQiYHBFREREREQkAgZXREREREREImBwRUREREREJAIGV0RERERERCJgcEVERERERCQCBldEREREREQi+H/lngHw2YfIOAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "jXzZETvasFM3"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}