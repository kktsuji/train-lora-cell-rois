{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOdEdcA9EngMwcHcR3YmEFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b7c99c3c4d542c7b163f8a2c6c7bb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d9a5a3311c242bd8146be0f74c825c2",
              "IPY_MODEL_c840b027bc154ebfbff67f077f184358",
              "IPY_MODEL_981627341e9c4dfb9e13cb78ad033abf"
            ],
            "layout": "IPY_MODEL_eefc46c87c4f48b7b00ceabc2442f907"
          }
        },
        "8d9a5a3311c242bd8146be0f74c825c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c5626a68e6475f97f4b1465ef9c61e",
            "placeholder": "​",
            "style": "IPY_MODEL_dc07fcff40c94396b30a4e82bbba3c21",
            "value": "Steps:  14%"
          }
        },
        "c840b027bc154ebfbff67f077f184358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97dfcebf2c84ca78d429d45ab9546b3",
            "max": 3700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c81316346df44213a2519f487821171c",
            "value": 518
          }
        },
        "981627341e9c4dfb9e13cb78ad033abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777dd872c1b94dfdba2e5de455b3a6c5",
            "placeholder": "​",
            "style": "IPY_MODEL_bb2d565ad14b48949eba590cae106d0e",
            "value": " 518/3700 [04:40&lt;27:46,  1.91it/s, loss=0.0415, lr=9.53e-5]"
          }
        },
        "eefc46c87c4f48b7b00ceabc2442f907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c5626a68e6475f97f4b1465ef9c61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc07fcff40c94396b30a4e82bbba3c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d97dfcebf2c84ca78d429d45ab9546b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c81316346df44213a2519f487821171c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "777dd872c1b94dfdba2e5de455b3a6c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb2d565ad14b48949eba590cae106d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kktsuji/train-lora-cell-rois/blob/main/train_lora_cell_rois.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPzC39ZGci3G",
        "outputId": "13240dd2-237a-4db0-e125-72bd878cf4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Settings\n",
        "\n",
        "# RESOLUTION = \"40\" # original size\n",
        "# RESOLUTION = \"256\"\n",
        "RESOLUTION = \"512\" # recommended for sd-1.x\n",
        "\n",
        "NUM_TRAIN_EPOCHS = \"100\"\n",
        "\n",
        "out_dir_name = f\"cell-rois_lora_resolution-{RESOLUTION}_epochs-{NUM_TRAIN_EPOCHS}\""
      ],
      "metadata": {
        "id": "aOMYXJ_44urC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "out_dir_path = f\"/content/drive/MyDrive/Research/LoRAs/{out_dir_name}/\"\n",
        "log_dir_path = f\"/content/drive/MyDrive/Research/LoRAs/{out_dir_name}/logs/\"\n",
        "if not os.path.exists(out_dir_path):\n",
        "  os.makedirs(out_dir_path)\n",
        "if not os.path.exists(log_dir_path):\n",
        "  os.makedirs(log_dir_path)\n",
        "print(out_dir_path)\n",
        "print(log_dir_path)\n",
        "\n",
        "base_data_dir_path = \"/content/drive/MyDrive/Research/Data/250606_cell_rois/pseudo_rgb/\"\n",
        "print(base_data_dir_path)\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "model_dir_path = \"/content/drive/MyDrive/Research/Models/\" + model_id\n",
        "print(model_dir_path)\n",
        "\n",
        "if (os.path.exists(out_dir_path) or\n",
        "  os.path.exists(log_dir_path) or\n",
        "  os.path.exists(base_data_dir_path) or\n",
        "  os.path.exists(model_dir_path)):\n",
        "  print(\"All directories exists\")\n",
        "else:\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()\n",
        "  raise Exception(\"Directory not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kv_xTpGeVqE",
        "outputId": "8a84bb48-b2be-4bf6-e1a8-9f4b35219bf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/LoRAs/cell-rois_lora_resolution-512_epochs-100/\n",
            "/content/drive/MyDrive/Research/LoRAs/cell-rois_lora_resolution-512_epochs-100/logs/\n",
            "/content/drive/MyDrive/Research/Data/250606_cell_rois/pseudo_rgb/\n",
            "/content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4\n",
            "All directories exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(model_dir_path):\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  import torch\n",
        "  os.makedirs(model_dir_path)\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
        "  pipe.save_pretrained(model_dir_path)"
      ],
      "metadata": {
        "id": "Plomt4WcKFHp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b develop https://github.com/kktsuji/peft.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('./peft/examples/stable_diffusion')\n",
        "import train_dreambooth\n",
        "\n",
        "print(train_dreambooth.UNET_TARGET_MODULES)\n",
        "\n",
        "# for \"CompVis/stable-diffusion-v1-4\"\n",
        "unet_target_modules = [\n",
        "    # Convolution layers\n",
        "    \"conv_in\", \"conv1\", \"conv2\", \"conv_out\",\n",
        "    \"conv_shortcut\", \"conv\",\n",
        "    # Linear layers\n",
        "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",\n",
        "    \"linear_1\", \"linear_2\", \"proj_in\", \"proj_out\", \"proj\",\n",
        "    \"time_emb_proj\", \"ff.net.0.proj\", \"ff.net.2\",\n",
        "]\n",
        "\n",
        "train_dreambooth.UNET_TARGET_MODULES = unet_target_modules\n",
        "print(train_dreambooth.UNET_TARGET_MODULES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01v6bCHo-Zgb",
        "outputId": "d1c5eec5-ea34-439d-9970-5e28caa646b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'peft'...\n",
            "remote: Enumerating objects: 11539, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
            "remote: Total 11539 (delta 153), reused 70 (delta 70), pack-reused 11321 (from 2)\u001b[K\n",
            "Receiving objects: 100% (11539/11539), 16.99 MiB | 32.28 MiB/s, done.\n",
            "Resolving deltas: 100% (7896/7896), done.\n",
            "['to_q', 'to_k', 'to_v', 'proj', 'proj_in', 'proj_out', 'conv', 'conv1', 'conv2', 'conv_shortcut', 'to_out.0', 'time_emb_proj', 'ff.net.2']\n",
            "['conv_in', 'conv1', 'conv2', 'conv_out', 'conv_shortcut', 'conv', 'to_q', 'to_k', 'to_v', 'to_out.0', 'linear_1', 'linear_2', 'proj_in', 'proj_out', 'proj', 'time_emb_proj', 'ff.net.0.proj', 'ff.net.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_args = [\n",
        "    \"--pretrained_model_name_or_path\", model_dir_path,\n",
        "    \"--instance_data_dir\", base_data_dir_path,\n",
        "    \"--instance_prompt\", \"rds\",\n",
        "    \"--seed\", \"0\",\n",
        "    \"--resolution\", RESOLUTION,\n",
        "    \"--output_dir\", out_dir_path,\n",
        "    \"--num_train_epochs\", NUM_TRAIN_EPOCHS,\n",
        "    \"--lr_scheduler\", \"cosine\",\n",
        "    \"--lr_warmup_steps\", \"5\",\n",
        "    \"--learning_rate\", \"1e-4\",\n",
        "    \"--logging_dir\", \"logs\",\n",
        "    \"--report_to\", \"tensorboard\",\n",
        "    \"lora\",\n",
        "    \"--unet_r\", \"16\",\n",
        "    \"--unet_alpha\", \"16\"\n",
        "]\n",
        "\n",
        "args = train_dreambooth.parse_args(input_args)\n",
        "train_dreambooth.main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4b7c99c3c4d542c7b163f8a2c6c7bb7e",
            "8d9a5a3311c242bd8146be0f74c825c2",
            "c840b027bc154ebfbff67f077f184358",
            "981627341e9c4dfb9e13cb78ad033abf",
            "eefc46c87c4f48b7b00ceabc2442f907",
            "04c5626a68e6475f97f4b1465ef9c61e",
            "dc07fcff40c94396b30a4e82bbba3c21",
            "d97dfcebf2c84ca78d429d45ab9546b3",
            "c81316346df44213a2519f487821171c",
            "777dd872c1b94dfdba2e5de455b3a6c5",
            "bb2d565ad14b48949eba590cae106d0e"
          ]
        },
        "id": "BGQ8iM4g7OoX",
        "outputId": "3e1cd3fa-2949-4900-f088-1ca6f220f176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /content/drive/MyDrive/Research/Models/CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 16,931,456 || all params: 876,452,420 || trainable%: 1.9318\n",
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): UNet2DConditionModel(\n",
            "      (conv_in): lora.Conv2d(\n",
            "        (base_layer): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Identity()\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "        (lora_magnitude_vector): ModuleDict()\n",
            "      )\n",
            "      (time_proj): Timesteps()\n",
            "      (time_embedding): TimestepEmbedding(\n",
            "        (linear_1): lora.Linear(\n",
            "          (base_layer): Linear(in_features=320, out_features=1280, bias=True)\n",
            "          (lora_dropout): ModuleDict(\n",
            "            (default): Identity()\n",
            "          )\n",
            "          (lora_A): ModuleDict(\n",
            "            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "          )\n",
            "          (lora_B): ModuleDict(\n",
            "            (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "          )\n",
            "          (lora_embedding_A): ParameterDict()\n",
            "          (lora_embedding_B): ParameterDict()\n",
            "          (lora_magnitude_vector): ModuleDict()\n",
            "        )\n",
            "        (act): SiLU()\n",
            "        (linear_2): lora.Linear(\n",
            "          (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora_dropout): ModuleDict(\n",
            "            (default): Identity()\n",
            "          )\n",
            "          (lora_A): ModuleDict(\n",
            "            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "          )\n",
            "          (lora_B): ModuleDict(\n",
            "            (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "          )\n",
            "          (lora_embedding_A): ParameterDict()\n",
            "          (lora_embedding_B): ParameterDict()\n",
            "          (lora_magnitude_vector): ModuleDict()\n",
            "        )\n",
            "      )\n",
            "      (down_blocks): ModuleList(\n",
            "        (0): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=5120, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): DownBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (up_blocks): ModuleList(\n",
            "        (0): UpBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-2): 3 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(2560, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=5120, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1920, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=640, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_k): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_v): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): lora.Linear(\n",
            "                          (base_layer): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                          (lora_dropout): ModuleDict(\n",
            "                            (default): Identity()\n",
            "                          )\n",
            "                          (lora_A): ModuleDict(\n",
            "                            (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                          )\n",
            "                          (lora_B): ModuleDict(\n",
            "                            (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                          )\n",
            "                          (lora_embedding_A): ParameterDict()\n",
            "                          (lora_embedding_B): ParameterDict()\n",
            "                          (lora_magnitude_vector): ModuleDict()\n",
            "                        )\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(960, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (1-2): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (time_emb_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): lora.Conv2d(\n",
            "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): lora.Conv2d(\n",
            "                (base_layer): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Conv2d(640, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Conv2d(16, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (mid_block): UNetMidBlock2DCrossAttn(\n",
            "        (attentions): ModuleList(\n",
            "          (0): Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_k): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_k): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Identity()\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=10240, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                        (lora_magnitude_vector): ModuleDict()\n",
            "                      )\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): lora.Linear(\n",
            "                      (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=5120, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                      (lora_magnitude_vector): ModuleDict()\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (time_emb_proj): lora.Linear(\n",
            "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): lora.Conv2d(\n",
            "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Identity()\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Conv2d(1280, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Conv2d(16, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "      (conv_act): SiLU()\n",
            "      (conv_out): lora.Conv2d(\n",
            "        (base_layer): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Identity()\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "        (lora_magnitude_vector): ModuleDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3700 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b7c99c3c4d542c7b163f8a2c6c7bb7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4151\n",
            "GPU Memory consumed at the end of the train (end-begin): 163\n",
            "GPU Peak Memory consumed during the train (max-begin): 7554\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 1573\n",
            "CPU Memory consumed at the end of the train (end-begin): 731\n",
            "CPU Peak Memory consumed during the train (max-begin): 743\n",
            "CPU Total Peak Memory consumed during the train (max): 2316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2305\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2305\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2305\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory before entering the train : 4315\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 7390\n",
            "GPU Total Peak Memory consumed during the train (max): 11705\n",
            "CPU Memory before entering the train : 2306\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 12\n",
            "CPU Total Peak Memory consumed during the train (max): 2318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "jXzZETvasFM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}